{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Analyzing Forest Vegetation Anomalies Using Sentinel-2 Zarr Data Cubes\" ### Add here the title of the notebook as displayed on the left-side menu\n",
    "execute:\n",
    "  enabled: false\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a href=\"https://jupyterhub.user.eopf.eodc.eu/hub/user-redirect/git-pull?repo=https://github.com/eopf-toolkit/eopf-101&branch=main&urlpath=lab/tree/eopf-101/06_eopf_zarr_in_action/65_vegetation_anomalies\" target=\"_blank\">\n",
    "  <button style=\"background-color:#0072ce; color:white; padding:0.6em 1.2em; font-size:1rem; border:none; border-radius:6px; margin-top:1em;\">\n",
    "    ðŸš€ Launch this notebook in JupyterLab\n",
    "  </button>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "**By:** *[@davemlz](https://github.com/davemlz)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "Forests are the largest terrestrial carbon sinks, playing a critical role in regulating the global carbon cycle through sustained CO$_2$ uptake. However, extreme climate events (such as heatwaves and droughts) can disrupt forest functioning, reducing photosynthetic activity and, in severe cases, causing tree mortality.\n",
    "\n",
    "While in situ measurements provide valuable information on forest health, collecting such data over large areas is costly, time-consuming, and logistically challenging. Scalable and continuous monitoring therefore requires more efficient approaches.\n",
    "\n",
    "In this case study, we will explore how to use **Sentinel-2 L2A data stored as Zarr data cubes** to analyze vegetation anomalies in forest ecosystems in Germany. Specifically, we will focus on two ICOS sites affected by the drought of 2018: [DE-Hai](https://meta.icos-cp.eu/resources/stations/ES_DE-Hai) and [DE-Tha](https://meta.icos-cp.eu/resources/stations/ES_DE-Tha), with DE-Tha left for learners to explore on their own.\n",
    "\n",
    "By leveraging **spatiotemporal data cubes**, we will compute spectral indices and derive anomaly time series to evaluate forest responses to extreme events in CO$_2$ uptake (Gross Primary Production, GPP, from ICOS). This notebook will guide you through a **modular workflow** for:\n",
    "\n",
    "1. Accessing Sentinel-2 Zarr data from STAC.  \n",
    "2. Calculating vegetation indices.  \n",
    "3. Detecting anomalies in time series.  \n",
    "4. Visualizing forest responses to environmental extremes.\n",
    "\n",
    "Through this study case, you will see the potential of **Zarr-based data cubes** for scalable forest monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### What we will learn\n",
    "\n",
    "In this notebook, you will gain hands-on experience with the following:\n",
    "\n",
    "- ðŸš€ **Creating Sentinel-2 L2A data cubes** from the [EOPF Zarr STAC](https://stac.core.eopf.eodc.eu/).  \n",
    "- ðŸ”Ž **Operating on data cubes**, including resampling and interpolation.  \n",
    "- ðŸŒ¿ **Computing spectral indices** using [Awesome Spectral Indices](https://github.com/awesome-spectral-indices/awesome-spectral-indices).  \n",
    "- ðŸ“ˆ **Deriving vegetation anomalies** from time series data.  \n",
    "- ðŸ“Š **Visualizing time series** and comparing results against GPP measurements from [ICOS](https://www.icos-cp.eu/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "To analyze vegetation anomalies, we will work with **vegetation indices**. To leverage the full power of `xarray`, we will use the **`spyndex` Python package**, which provides a Python API for the **Awesome Spectral Indices** catalogue. This gives access to **over 200 spectral indices**, which can be computed directly on various Python data types, including `xarray` datasets and data arrays.  \n",
    "\n",
    "For more details, refer to the [Awesome Spectral Indices paper in *Scientific Data*](https://doi.org/10.1038/s41597-023-02096-0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dask\n",
    "import spyndex\n",
    "\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Helper functions were defined in the `vegetation_anomalies_utils.py` file. Here we import all of the functions and explain their role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vegetation_anomalies_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "##### `get_items`\n",
    "\n",
    "This function **queries the EOPF STAC API** for **Sentinel-2 L2A items** that intersect a specified latitude/longitude point within a given **date range**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "##### `latlon_to_buffer_bbox`\n",
    "\n",
    "This function **converts a latitude/longitude point** to a specified **projected coordinate system (EPSG)** and then generates a **square bounding box** centered on that point.\n",
    "\n",
    "This helper is not used directly as it is incorporated in the next helper function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "##### `open_and_curate_data`\n",
    "\n",
    "This function **opens a Sentinel-2 STAC item (Zarr)**, performs a **spatial subset** around a given point, applies **SCL-based masking**, and returns an **`xarray.Dataset`** containing only the selected reflectance bands (e.g. `B04`, `B8A`) with a **time dimension**. It uses the `validate_scl` function from `Zarr_wf_utils.py`.\n",
    "\n",
    "> **Note:** This function is a **delayed Dask object**, meaning it will only be executed when explicitly triggered through Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "##### `curate_gpp`\n",
    "\n",
    "This function **loads a GPP time series**, computes **weekly anomalies**, identifies **extreme low-GPP events**, and filters these extremes to retain only events lasting at least a specified number of **consecutive weeks**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Vegetation Anomalies in the Hainich National Park"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "We will start the notebook with a forest ecosystem that was severely affacted by the drought of 2018: DE-Hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE-Hai Coordinates\n",
    "HAI_LAT = 51.079212\n",
    "HAI_LON = 10.452168"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Initialize a **Dask distributed client** to enable parallel and delayed computation. This will manage the execution of tasks, such as loading and processing large Sentinel-2 Zarr datasets, efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Load the GPP data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Load the **GPP time series** for the DE-Hai site and compute **weekly anomalies**, identifying extreme low-GPP events.\n",
    "\n",
    "- The following helper function just needs one of two possible inputs: \"DE-Hai\" (this example), or \"DE-Tha\" (for learners to explore).\n",
    "\n",
    "The obtained dataframe contains the following columns:\n",
    "\n",
    "- `GPP_NT_VUT_REF`: Gross Primary Production (GPP) values per week.\n",
    "- `weekofyear`: Number of the week of the year [1-53].\n",
    "- `anomaly`: GPP anomalies, computed as the deviation from the mean seasonal cycle.\n",
    "- `extreme`: Whether the specific value was a extreme or not (1 = extreme, 0 = normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAI_df = curate_gpp(\"DE-Hai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Create the Sentinel-2 L2A Data Cube\n",
    "\n",
    "First, query the **EOPF STAC API** to retrieve Sentinel-2 L2A items for the DE-Hai site.\n",
    "\n",
    "- We specify for the following helper function the coordinates of the site (`lat`, `lon`), and we set `return_as_dicts` to *True* to avoid ocnflicts when a *href* link is not present in an asset of the retrieved items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all items as a list of dicts\n",
    "HAI_items = get_items(HAI_LAT,HAI_LON,return_as_dicts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "For each item, **open and curate** the data by subsetting around the site coordinates and selecting the relevant bands:\n",
    "\n",
    "- The `item` parameter is a dictionary obtained from each item in `HAI_items`.\n",
    "- We specify the coordinates of the site (`lat`, `lon`).\n",
    "- The `bands` parameter is a list of the bands you want to retrive, in this case we just want the red and near-infrared bands (b04 and b8a).\n",
    "- The `resolution` we will use is 20 m: This must be one of 10, 20 or 60.\n",
    "- The `buffer` parameter us set to 500 m (a bounding box is constructed from this parameter as well as from the `lat`, `lon` parameters).\n",
    "- Finally, `items_as_dicts` is set to *True* since we defined it in the previous code cell.\n",
    "\n",
    "After this, datasets are **computed in parallel with Dask**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the delayed Dask objects\n",
    "HAI_ds = [open_and_curate_data(item,lat=HAI_LAT,lon=HAI_LON,bands=[\"b04\",\"b8a\"],resolution=20,buffer=500,items_as_dicts=True) for item in HAI_items]\n",
    "\n",
    "# Compute the delayed objects in parallel. This outputs a list of xarray.Dataset objects\n",
    "HAI_data = dask.compute(*HAI_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "After computed, datasets are concatenated along the **time dimension**, sorted by time, and finally **loaded into memory** as a single `xarray.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the previous list using the time dimension and sort it\n",
    "HAI_ds = xr.concat(HAI_data,dim=\"time\").sortby(\"time\")\n",
    "\n",
    "# Load it into memory\n",
    "HAI_ds = HAI_ds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Compute Vegetation Indices\n",
    "\n",
    "Compute **spectral indices** for the DE-Hai dataset using the [`spyndex` package](https://github.com/awesome-spectral-indices/spyndex). In this example, we calculate **NDVI** and **kernel NDVI (kNDVI)**.\n",
    "\n",
    "- [**NDVI (Normalized Difference Vegetation Index)**](https://ntrs.nasa.gov/citations/19740022614) is a widely used index to monitor vegetation health and greenness. It is calculated as:\n",
    "\n",
    "$$\\text{NDVI} = \\frac{N - R}{N + R}$$\n",
    "\n",
    "where `N` is the near-infrared band (`B8A`) and `R` is the red band (`B04`).\n",
    "\n",
    "- [**kNDVI (Kernel NDVI)**](https://doi.org/10.1126/sciadv.abc7447) is a kernelized version of NDVI. Here, an **RBF (Radial Basis Function) kernel** is applied:\n",
    "\n",
    "$$\\text{kNDVI} = \\frac{k(N,N) - k(N,R)}{k(N,N) + k(N,R)}$$\n",
    "\n",
    "where $k(a,b)$ is the RBF kernel:\n",
    "\n",
    "$$k(N,N) = 1$$\n",
    "\n",
    "$$k(N,R) = \\exp\\left(-\\frac{(N-R)^2}{2 \\sigma^2}\\right)$$\n",
    "\n",
    "Here, $\\sigma$ is claculated as the median in the time dimension of $0.5(N+R)$.\n",
    "\n",
    "The `spyndex.computeIndex` function computes both indices across the time series and stores them in an **`xarray.Dataset`** named `idx` for subsequent anomaly analysis.\n",
    "\n",
    "The `spyndex.computeKernel` function computes the kernel for the kNDVI.\n",
    "\n",
    "Note that in both cases `spyndex` just requires the data for computing the indices without the need of hard-coding the formulas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = spyndex.computeIndex(\n",
    "    [\"NDVI\",\"kNDVI\"], # Indices to compute\n",
    "    N = HAI_ds[\"b8a\"], # NIR band\n",
    "    R = HAI_ds[\"b04\"], # Red band\n",
    "    kNN = 1.0,\n",
    "    kNR = spyndex.computeKernel(\n",
    "        \"RBF\", # RBF kernel\n",
    "        a = HAI_ds[\"b8a\"],\n",
    "        b = HAI_ds[\"b04\"],\n",
    "        sigma = ((HAI_ds[\"b8a\"] + HAI_ds[\"b04\"])/2.0).median(\"time\")\n",
    "    )\n",
    ").to_dataset(\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Add the name and units of each index to the attributes according to the CF conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx.NDVI.attrs[\"long_name\"] = spyndex.indices.NDVI.long_name\n",
    "idx.NDVI.attrs[\"units\"] = \"1\"\n",
    "\n",
    "idx.kNDVI.attrs[\"long_name\"] = spyndex.indices.kNDVI.long_name\n",
    "idx.kNDVI.attrs[\"units\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Resample the NDVI and kNDVI time series to **weekly frequency**, taking the **median** within each week. After resampling, fill temporal gaps by applying **cubic interpolation** along the time dimension. This produces smooth, continuous weekly index time series suitable for anomaly computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = idx.resample(time=\"1W\").median().interpolate_na(dim=\"time\",method=\"cubic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Calculate Vegetation Anomalies\n",
    "\n",
    "Compute the **median seasonal cycle** (MSC) for NDVI and kNDVI.\n",
    "\n",
    "By grouping the time series by `weekofyear` and taking the **median across years**, this step produces a climatological baseline that represents the typical vegetation state for each week of the year.  \n",
    "\n",
    "The MSC will be used later to derive vegetation anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc = idx.groupby(\"time.weekofyear\").median(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Plot the MSC of the NDVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc.NDVI.plot.imshow(col = \"weekofyear\",cmap = \"viridis\",col_wrap = 8,vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Plot the MSC of the kNDVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc.kNDVI.plot.imshow(col = \"weekofyear\",cmap = \"viridis\",col_wrap = 8,vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Compute **vegetation anomalies** by subtracting the **median seasonal cycle (MSC)** from the weekly NDVI and kNDVI values. This step isolates deviations from the expected seasonal pattern, allowing us to identify abnormal vegetation conditions potentially linked to stress or extreme events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_anomalies = idx.groupby(\"time.weekofyear\") - msc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Add the name and units of each index anomaly to the attributes according to the CF conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_anomalies.NDVI.attrs[\"long_name\"] = spyndex.indices.NDVI.long_name + \" Anomaly\"\n",
    "idx_anomalies.NDVI.attrs[\"units\"] = \"1\"\n",
    "\n",
    "idx_anomalies.kNDVI.attrs[\"long_name\"] = spyndex.indices.kNDVI.long_name + \" Anomaly\"\n",
    "idx_anomalies.kNDVI.attrs[\"units\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Visualize Time Series\n",
    "\n",
    "We will use time series for visualization. Let's first aggregate the indices in space using the median to produce a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_agg = idx.median([\"x\",\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Here, we defined the colors to use for our indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for indices\n",
    "NDVI_COLOR = 'limegreen'\n",
    "kNDVI_COLOR = 'darkviolet'\n",
    "\n",
    "# Color for zero line in anomalies\n",
    "ZERO_COLOR = 'red'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Now, we will plot the NDVI and kNDVI time series together with the GPP measurements for the DE-Hai site. A secondary axis will be used to display GPP, allowing direct visual comparison between vegetation dynamics and ecosystem productivity. Extreme low-GPP events are highlighted as shaded red intervals: These events are defined as **periods of at least two consecutive days** in which GPP anomalies fall **below the 10th percentile** of the lower tail of the distribution. This information is contained in the `HAI_df` dataframe created via `curate_gpp` helper function.\n",
    "\n",
    "This visualization helps link vegetation index to observed reductions in carbon uptake, revealing how forest canopy responses relate to ecosystem-scale stress signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "\n",
    "ax.plot(idx_agg.time, idx_agg[\"NDVI\"],  color=NDVI_COLOR,  label=\"NDVI\")\n",
    "ax.plot(idx_agg.time, idx_agg[\"kNDVI\"], color=kNDVI_COLOR, label=\"kNDVI\")\n",
    "ax.set_ylim([-0.15,1.2])\n",
    "ax.set_ylabel(\"VI\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(df.index, df[\"GPP_NT_VUT_REF\"], \n",
    "            s=20, color=\"grey\", alpha=0.6, label=\"GPP\")\n",
    "ax2.set_ylim([-3.5,17.5])\n",
    "ax2.set_ylabel(\"GPP\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "extreme_mask = df[\"extreme\"] == 1\n",
    "groups = (extreme_mask != extreme_mask.shift()).cumsum()\n",
    "\n",
    "for _, group in df[extreme_mask].groupby(groups):\n",
    "    start = group.index.min()\n",
    "    end   = group.index.max()\n",
    "    ax.axvspan(start, end, color=\"red\", alpha=0.15)\n",
    "\n",
    "plt.title(\"NDVI, kNDVI, GPP and Extreme Events\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Now, let's do the same for the anomalies by aggregating the anomalies of the indices in space using the median to produce a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_anomalies_agg = idx_anomalies.median([\"x\",\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "Now we can visualize the **anomaly time series** of NDVI, kNDVI, and GPP for the DE-Hai site. Here, both vegetation indices and GPP have been transformed into **weekly anomalies**, representing deviations from their typical seasonal cycles. A horizontal line at zero indicates the expected baseline. A secondary axis displays **GPP anomalies**, allowing direct comparison between canopy-level spectral responses and ecosystem-level carbon uptake changes. Extreme low-GPP events are shown as shaded red intervals.  \n",
    "\n",
    "This plot highlights how vegetation index anomalies co-occur with (e.g. 2021) or diverge (e.g. 2018) from carbon uptake anomalies, offering insight into forest responses to stress events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "\n",
    "ax.plot(idx_anomalies_agg.time, idx_anomalies_agg[\"NDVI\"],  color=NDVI_COLOR,  label=\"NDVI\")\n",
    "ax.plot(idx_anomalies_agg.time, idx_anomalies_agg[\"kNDVI\"], color=kNDVI_COLOR, label=\"kNDVI\")\n",
    "ax.axhline(0, color=ZERO_COLOR, linewidth=1)\n",
    "ax.set_ylim([-0.45,0.45])\n",
    "ax.set_ylabel(\"VI Anomaly\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(df.index, df[\"anomaly\"], \n",
    "            s=20, color=\"grey\", alpha=0.6, label=\"GPP\")\n",
    "ax2.set_ylim([-6.5,6.5])\n",
    "ax2.set_ylabel(\"GPP Anomaly\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "extreme_mask = df[\"extreme\"] == 1\n",
    "groups = (extreme_mask != extreme_mask.shift()).cumsum()\n",
    "\n",
    "for _, group in df[extreme_mask].groupby(groups):\n",
    "    start = group.index.min()\n",
    "    end   = group.index.max()\n",
    "    ax.axvspan(start, end, color=\"red\", alpha=0.15)\n",
    "\n",
    "plt.title(\"NDVI, kNDVI, GPP Anomalies and Extreme Events\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## ðŸ’ª Now it is your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "The following exercises will help you reproduce the previous workflow for another dataset.\n",
    "\n",
    "### Task 1: Create a data cube for DE-Tha\n",
    "* Use the coordinates of DE-Tha (provided in the cell below) to create a data cube for this site.\n",
    "* Retrieve the red edge bands in addition to the NIR and red bands.\n",
    "* Modify the code as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE-Tha Coordinates\n",
    "THA_LAT = 50.9625\n",
    "THA_LON = 13.56515\n",
    "\n",
    "# Get all items as a list of dicts\n",
    "# THA_items = get_items(THA_LAT,THA_LON,return_as_dicts=True)\n",
    "\n",
    "# Create the delayed Dask objects\n",
    "# THA_ds = [open_and_curate_data(..., bands=[\"b04\", \"b05\", \"b06\", \"b07\", \"b8a\"]) for ...]\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Task 2: Compute Vegetation Indices\n",
    "* Select a vegetation index from [Awesome Spectral Indices](https://github.com/awesome-spectral-indices/awesome-spectral-indices) that uses the Red Edge bands.\n",
    "* Modify the code as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices that include any of the red edge bands\n",
    "for idx, attrs in spyndex.indices.items():\n",
    "    if any(item in [\"RE1\",\"RE2\",\"RE3\"] for item in attrs.bands):\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### Task 3: Calculate Vegetation Anomalies\n",
    "* Calculate anomalies for the selected index and compare them against the GPP anomalies of DE-Tha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "THA_df = curate_gpp(\"DE-Tha\")\n",
    "\n",
    "# THA_df[\"GPP_NT_VUT_REF\"] -> GPP values\n",
    "# THA_df[\"time\"] -> time simension\n",
    "# THA_df[\"anomaly\"] -> Anomalies\n",
    "# THA_df[\"extreme\"] -> Extreme = 1, Normal condition = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "In this notebook, we explored how **Sentinel-2 L2A Zarr data cubes** can be used to monitor forest vegetation dynamics and detect anomalous behavior linked to ecosystem stress. By leveraging Zarr, STAC-based discovery, and `xarray`/`Dask` for scalable computation, we built an end-to-end workflow that included:\n",
    "\n",
    "- Accessing Sentinel-2 data from the EOPF Zarr STAC  \n",
    "- Creating spatiotemporal data cubes centered on forest monitoring sites  \n",
    "- Computing spectral indices (NDVI, kNDVI) using the Awesome Spectral Indices catalogue  \n",
    "- Constructing weekly time series and climatological baselines  \n",
    "- Deriving vegetation anomalies and comparing them with GPP anomalies from ICOS  \n",
    "- Identifying and visualizing extreme low-GPP events\n",
    "\n",
    "Through the joint analysis of **spectral indices** and **ecosystem productivity**, we demonstrated how remote sensing can reveal (or not) signals of forest stress and complement flux tower observations. This workflow illustrates the value of **Zarr-based EO data**, **open standards (STAC)**, and **modern geospatial Python tools** for reproducible and scalable environmental monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "We would like to thank ICOS for providing the data on the Ecosystem stations DE-Hai [1] and DE-Tha [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Knohl, A., Schulze, E.-D., Kolle, O., & Buchmann, N. (2003). Large carbon uptake by an unmanaged 250-year-old deciduous forest in Central Germany. Agricultural and Forest Meteorology, 118(3â€“4), 151â€“167. https://doi.org/10.1016/s0168-1923(03)00115-1\n",
    "\n",
    "[2] GrÃ¼nwald, T., & Bernhofer, C. (2007). A decade of carbon, water and energy flux measurements of an old spruce forest at the Anchor Station Tharandt. Tellus B: Chemical and Physical Meteorology, 59(3), 387. https://doi.org/10.1111/j.1600-0889.2007.00259.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "In the following [notebook](./69_coastal_water_dynamics_s1.ipynb), we will explore how to monitor surface water dynamics in coastal wetlands using Sentinel-1 time series.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
