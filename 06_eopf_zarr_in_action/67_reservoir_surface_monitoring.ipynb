{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Reservoir Surface Monitoring\"\n",
    "execute:\n",
    "  enabled: true\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a href= 'https://jupyterhub.user.eopf.eodc.eu/hub/login?next=/hub/spawn?next=/hub/user-redirect/git-pull?repo=https://github.com/eopf-toolkit/eopf-101&branch=main&urlpath=lab/tree/eopf-101/02_about_eopf_zarr/67_reservoir_surface_monitoring.ipynb#fancy-forms-config={\"profile\":\"choose-your-environment\",\"image\":\"unlisted_choice\",\"image:unlisted_choice\":\"4zm3809f.c1.de1.container-registry.ovh.net/eopf-toolkit-python/eopf-toolkit-python:latest\",\"autoStart\":\"true\"}' target=\"_blank\">\n",
    "  <button style=\"background-color:#0072ce; color:white; padding:0.6em 1.2em; font-size:1rem; border:none; border-radius:6px; margin-top:1em;\">\n",
    "    ðŸš€ Launch this notebook in JupyterLab\n",
    "  </button>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "**By:** *[@atsiokanos](https://github.com/atsiokanos)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Water reservoirs are essential for water supply, energy production, and irrigation. However, population growth, economic expansion, and climate change are increasing pressure on these resources, affecting water availability and raising the risk of droughts and floods. Reliable monitoring of reservoirs is critical to ensure sustainable water management and water security. This is especially important in transboundary basins, where upstream reservoirs can significantly influence water availability for downstream countries.\n",
    "\n",
    "The [Global Water Watch (GWW)](https://www.globalwaterwatch.earth/) is a platform developed by [Deltares](https://www.deltares.nl/en) with support from [Google.org](https://www.google.org/), the [Water, Peace, and Security Partnership](https://waterpeacesecurity.org/), and the [European Space Agency (ESA)](https://www.esa.int/). It provides near-real-time (NRT), globally accessible information on reservoirs using Earth Observation data, helping stakeholders monitor changes in water extent and manage resources more effectively. A detailed description of the GWW methods is available under *High-resolution surface water dynamics in Earthâ€™s small and medium-sized reservoirs* by Donchyts et al., [here](https://www.nature.com/articles/s41598-022-17074-6).  \n",
    "\n",
    "In this notebook, we **implement parts of the GWW algorithms** to estimate water surface area for a single reservoir: **Mita Hills in Zambia**. Although the original GWW algorithm uses **Landsat 7 & 8** as well as **Sentinel-2**, we focus only on **Sentinel-2 (L1C) imagery** here for simplicity. For the purpose of this resource, we use Sentinel-2 (L1C) imagery available through the [EOPF STAC Catalog](https://stac.browser.user.eopf.eodc.eu/?.language=en).\n",
    "\n",
    "**Section 1** focuses on preparing and preprocessing the Sentinel-2 data, while **Section 2** covers the steps required to estimate reservoir surface area using the GWW methodology.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### What we will learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "- How to retrieve Sentinel-2 L1C data from the EOPF STAC Catalog?\n",
    "- Preprocess Sentinel-2 L1C imagery and filter out low-quality scenes\n",
    "- Compute the Normalized Difference Water Index (NDWI) to highlight water bodies\n",
    "- How to apply the GWW algorithm to Sentinel-2 L1C data to generate water masks and fill missing water pixels (e.g., due to clouds or shadows) with auxiliary data?\n",
    "- To extract the largest connected component and estimate the water surface area\n",
    "- How to efficiently process large image collections using parallel computing techniques with **Dask**?\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This notebook uses Sentinel-2 L1C reflectance data from the EOPF STAC Catalog and requires a Water Occurrence dataset subset (provided [here](https://github.com/eopf-toolkit/eopf-101/tree/main/06_eopf_zarr_in_action/data)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel computing and data handling\n",
    "import dask                                  \n",
    "from dask.distributed import Client          \n",
    "\n",
    "# Data access\n",
    "from pystac_client import Client as StacClient   # Query EO datasets via STAC API\n",
    "from pystac import MediaType                     # Identify asset media types (e.g., ZARR)\n",
    "\n",
    "# Geospatial data processing\n",
    "from pyproj import CRS, Transformer            \n",
    "import xarray as xr                        \n",
    "import rioxarray\n",
    "from rasterio.enums import Resampling           \n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt                \n",
    "\n",
    "# Warnings management\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "This notebook utilizes a set of helper functions from the Python file [reservoir_surface_monitoring_utils.py](./reservoir_surface_monitoring_utils.py)  to handle key tasks, including loading and preprocessing the datasets, computing water masks using the GWW algorithm, and calculating reservoir areas. The main logic of each function is outlined in the notebook prior to its use, and the full code can be inspected in `reservoir_surface_monitoring_utils.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reservoir_surface_monitoring_utils import (\n",
    "    list_found_elements,\n",
    "    load_datatrees,\n",
    "    preprocess_datatree,\n",
    "    gww,\n",
    "    largest_connected_component,\n",
    "    compute_area_km2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Section 1 - Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Overview of Section 1**:\n",
    "\n",
    "**1.1 - Initial settings and data connection**    \n",
    "**1.2 - Load and preprocess data**  \n",
    "**1.3 - Filtering data**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 1.1 - Initial settings and data connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Initiate dask client for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start dask client \n",
    "client = Client()  # local cluster, can monitor dashboard\n",
    "print(client)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Define the settings and retrieve Sentinel-2 L1C data from the EOPF STAC Catalog. In this example, we will use the bounding box covering Mita Hills, apply a 30% cloud cover cutoff for the query, and select a period from August 2022 to December 2024. This relatively long period ensures a sufficient number of scenes, covering both dry and wet conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AOI bbox coordinates (minx, miny, maxx, maxy) in EPSG:4326 (WGS 84)\n",
    "minx, miny, maxx, maxy = 28.99253107, -14.2426452, 29.17046702, -13.95317373\n",
    "\n",
    "# Define time range\n",
    "start_date = \"2022-08-01\" \n",
    "end_date = \"2024-12-01\"\n",
    "\n",
    "# Define cloud cover cutoff percentile\n",
    "cloud_cutoff_percentile = 30  # e.g., 30%\n",
    "\n",
    "# Define collection\n",
    "collection = \"sentinel-2-l1c\"\n",
    "\n",
    "# Connect to EOPF STAC Catalog\n",
    "eopf_stac_api_root_endpoint = \"https://stac.core.eopf.eodc.eu/\"\n",
    "eopf_catalog = StacClient.open(url=eopf_stac_api_root_endpoint)\n",
    "\n",
    "# Start a search\n",
    "s2_collection = eopf_catalog.search(\n",
    "    collections=collection,\n",
    "    bbox=(minx, miny, maxx, maxy),\n",
    "    datetime=f\"{start_date}T00:00:00Z/{end_date}T23:59:59Z\",\n",
    "    query = {'eo:cloud_cover': {'lte': cloud_cutoff_percentile}}\n",
    ")\n",
    "\n",
    "# List found scenes\n",
    "scenes = list_found_elements(s2_collection)\n",
    "print(f\"Total items found for collection {collection} over AOI:\", len(scenes[0]))\n",
    "\n",
    "if len(scenes[0]) == 0:\n",
    "    raise ValueError(\"No scenes found for the specified AOI and time range. Please adjust your search parameters.\")\n",
    "\n",
    "# Retrieve ZARR URLs/paths\n",
    "S2l1c_coll = eopf_catalog.get_collection(collection)\n",
    "items_loc_url = []\n",
    "\n",
    "for item_id in scenes[0]:\n",
    "    item = S2l1c_coll.get_item(id=item_id)\n",
    "    item_assets = item.get_assets(media_type=MediaType.ZARR)\n",
    "    cloud_storage_url = item_assets['product'].href\n",
    "    items_loc_url.append(cloud_storage_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### 1.2 - Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "First we load the datasets using the `load_datatrees` function. Note that in the following cell loading is done lazily using `dask.delayed`, which builds a Dask task graph. Execution of this graph triggers **parallel processing** of all dataset loading tasks, improving efficiency and reducing memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datatrees in parallel using dask.delayed\n",
    "delayed_datatrees = [dask.delayed(load_datatrees)(path) for path in items_loc_url]\n",
    "datatrees = dask.compute(*delayed_datatrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "The obtained datatrees contain the full set of source bands (at all available resolutions) as well as useful metadata for every retrieved scene. Some preprocessing is necessary. This is handled by the `preprocess_datatree` function, which extracts only the bands of interest, i.e. green (b03) and NIR (b08), clips the tile to the AOI, merges the two bands into a single dataset, and finally adds a **time dimension** derived from the datatree attributes (\"stac_discovery\" / \"properties\" / \"start_datetime\").\n",
    "\n",
    "Although the search coordinates are given in geographic coordinates (EPSG:4326), the data retrieved from the **EOPF STAC API** is provided in UTM coordinates. Therefore, we need to convert the AOI coordinates to UTM as well, to ensure that the clipping is performed correctly. The image CRS code is also available in the datatree attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the scenes crs from the attributes of the first datatree\n",
    "crs_code = datatrees[0].attrs[\"other_metadata\"][\"horizontal_CRS_code\"]  \n",
    "print(\"Scene CRS code:\", crs_code)\n",
    "\n",
    "# Build CRS object and transformer from WGS84\n",
    "scene_crs = CRS.from_user_input(crs_code)\n",
    "project_to_scene = Transformer.from_crs(\"EPSG:4326\", scene_crs, always_xy=True)\n",
    "\n",
    "# Transform bounding box to scene CRS \n",
    "minx_utm, miny_utm = project_to_scene.transform(minx, miny)\n",
    "maxx_utm, maxy_utm = project_to_scene.transform(maxx, maxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Apply the preprocessing and compute the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess datatrees in parallel using dask.delayed\n",
    "delayed_datasets = [\n",
    "    dask.delayed(preprocess_datatree)(\n",
    "        dtree,\n",
    "        minx_utm,\n",
    "        miny_utm,\n",
    "        maxx_utm,\n",
    "        maxy_utm,\n",
    "    )\n",
    "    for dtree in datatrees\n",
    "]\n",
    "\n",
    "datasets = dask.compute(*delayed_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Create a datacube with all datasets along time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore some warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Converting non-nanosecond precision\")\n",
    "\n",
    "# Combine all datasets along time dimension\n",
    "data_cube = xr.concat(datasets, dim=\"time\")\n",
    "# Sort by time\n",
    "data_cube = data_cube.sortby(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### 1.3 - Filtering data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Although we selected scenes with less than 30% cloud coverage in our query, this threshold applies to the entire tile, meaning some clouds may still be present in the clipped `data_cube`. \n",
    "\n",
    "To address this, we first assign a quality score to each image using the green band by computing the 75th percentile (default in GWW) over the AOI (clouds have high reflectance so the lower the score the cleaner). \n",
    "\n",
    "We then keep only the cleanest scenes based on a chosen percentage of the total images (in our case 75%).\n",
    "\n",
    "Notes to consider: \n",
    "\n",
    "1) This step requires a long time series with a relatively large number of images (for example, in this notebook we use a period with around 100 images). If you are focusing on a shorter period, you can skip this subsection and move directly to Section 2.\n",
    "\n",
    "2) Selecting scenes with less than 30% cloud coverage can be somewhat arbitrary. A more accurate estimate could be obtained from climatology data, for example using the MODIS cloud occurrence dataset.\n",
    "\n",
    "3) Keeping the top 75% of the cleanest images is also somewhat arbitrary.\n",
    "\n",
    "4) All of the above filtering settings may reduce the number of final cleaned images compared to the number used/provided by the official GWW algorithm over the same period, although only minor differences are expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute quality scores based on green band 75th percentile\n",
    "quality_scores = data_cube[\"green\"].quantile(\n",
    "    q=0.75,\n",
    "    dim=(\"y\", \"x\"),\n",
    "    skipna=True\n",
    ")\n",
    "# Store quality scores in data cube\n",
    "data_cube[\"quality_score\"] = quality_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the 75% cleanest scenes based on quality scores\n",
    "keep_fraction = 0.75\n",
    "\n",
    "# Only compute the quality_score array\n",
    "qs = data_cube[\"quality_score\"].compute().values \n",
    "n_keep = max(int(len(qs) * keep_fraction), 1)\n",
    "\n",
    "# Indices of the n_keep smallest values\n",
    "sorted_idx = qs.argsort()[:n_keep]\n",
    "\n",
    "# Select those scenes from dataset \n",
    "data_cube = data_cube.isel(time=sorted_idx).sortby(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Finally, we have a clipped and cleaned `data_cube` containing all scenes, including a **time dimension** for the relevant bands as data variables (`time`, `lat`, `lon`). We will use this dataset in **Section 2** to perform time series analysis, namely to derive water extents and calculate the area for each scene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Section 2 - Reservoir Water Surface Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "**Overview of Section 2:**\n",
    "\n",
    "**2.1 - Compute Normalized Difference Water Index (NDWI)**    \n",
    "**2.2 - Integrate Water Occurrence (WO) data**  \n",
    "**2.3 - Generate water extents (core GWW algorithm)**  \n",
    "**2.4 - Extract Largest Connected Component**  \n",
    "**2.5 - Compute reservoir area**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### 2.1 - Compute Normalized Difference Water Index (NDWI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Once we have preprocessed and prepared the data for our AOI, we can start by computing the Normalized Difference Water Index (NDWI) using the green (b03) and NIR (b08) bands. The NDWI highlights water features by producing high positive values for water pixels and lower or negative values for land and vegetation. This provides an initial indication of where water is located, although simple thresholding at this stage can be unreliable, particularly near shorelines or in areas with shallow water or vegetation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate NDWI\n",
    "green = data_cube['green']\n",
    "nir = data_cube['nir']\n",
    "ndwi = (green - nir) / (green + nir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Visualize example output from Subsection 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot NDWI for a specific date as an example\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(ndwi.sel(time='2024-05-23'), vmin=-0.5, vmax=0.5, cmap='managua')\n",
    "plt.colorbar(label='NDWI', shrink=0.7)\n",
    "plt.title('NDWI on 2024-05-23');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "As shown in the plot, the water body area is visible, with high values represented by the deep sky-blue color. However, clouds are present within the water body area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### 2.2 - Integrate Water Occurrence (WO) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "The Global Surface Water Explorer (GSWE) dataset from the European Commission's Joint Research Centre is also used here. This dataset maps the location and temporal distribution of water surfaces at the global scale over the past 32 years, providing statistics on the extent and change of those water surfaces, namely water occurrence (WO) probabilities per pixel. WO represents the fraction of time a given pixel was observed as water over the study period. The table below shows the range of values and their interpretation.\n",
    "\n",
    "\n",
    "### Interpretation of Pixel Values in GSWE (Water Occurrence Layer)\n",
    "\n",
    "| **Pixel Value (range)** | **Interpretation** | **Typical Environment** |\n",
    "|--------------------------|--------------------|--------------------------|\n",
    "| 0.0                     | Never detected as water during the observation period | Land, desert, urban area |\n",
    "| 0.1 â€“ 0.5               | Occasionally detected as water (intermittent presence) | Seasonal ponds, floodplains, agricultural fields |\n",
    "| 0.5 â€“ 0.9               | Frequently water but not permanent | Wetlands, seasonal lakes, riverbanks |\n",
    "| ~1.0                    | Almost always water (permanent presence) | Lakes, large rivers, reservoirs |\n",
    "\n",
    "For more information the user is referred to the [associated journal article](https://www.nature.com/articles/nature20584) and the online [Data Users Guide](https://storage.googleapis.com/global-surface-water/downloads_ancillary/DataUsersGuidev2021.pdf).\n",
    "\n",
    "The WO dataset is static. For this exercise, we have stored a subset over our AOI (under the `data` folder), which we will read. In general, the dataset can be downloaded [here](https://global-surface-water.appspot.com/download) or accessed through **Google Earth Engine**. At the end of this notebook, in the example tasks, a code snippet is provided showing how the WO dataset can be downloaded and processed using `ee` and/or `geemap`.\n",
    "\n",
    "The **GSWE** will be used in the next step to perform some filling over the initially classified water areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_wo = rioxarray.open_rasterio(\"data/water_occurrence_utm35_v14.tif\")\n",
    "ndwi.rio.write_crs(crs_code, inplace=True)\n",
    "\n",
    "# resample it to match ndwi as it is on a 30m resolution\n",
    "ds_wo_matched = ds_wo.rio.reproject_match(\n",
    "    ndwi,\n",
    "    resampling=Resampling.nearest\n",
    ")\n",
    "ds_wo_matched = ds_wo_matched[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Visualize example output from Subsection 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot WO\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(ds_wo_matched, vmin=0, vmax=1, cmap='managua')\n",
    "plt.colorbar(label='Probability of Water Occurrence', shrink=0.7)\n",
    "plt.title('Water Occurrence');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "As shown in the plot, the main water body has very high probability values, close to one, indicating that these pixels were almost always water during the observation period, representing the permanent water areas of the reservoir. Lower values appear near the edges of the reservoir, which are the most challenging areas to detect. These lower probabilities typically indicate that water was present frequently but not permanently (values around 0.5â€“0.9) or only occasionally (values around 0.1â€“0.5). In the next subsection, we will focus on detecting these variable water areas in each image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### 2.3 - Generate water extents (core GWW algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "To detect water more reliably, we combine the following two datasets: the NDWI index and the Water Occurrence (WO).\n",
    "\n",
    "First, we identify edges in the NDWI image using the `Canny`, which highlights sharp transitions between water and land. After that, we apply a `morphological dilation` to the edges, and using only these dilated edge pixels, we apply `Otsu thresholding` to determine a robust cutoff value that separates water from land. This avoids biases from large uniform land areas and ensures the threshold is focused on transition zones. For more details on the Canny edge detector and Otsu thresholding see this [publication](http://www.mdpi.com/2072-4292/8/5/386).\n",
    "\n",
    "Next, we use the WO dataset to fill in water areas that NDWI might miss, such as shallow, turbid zones etc. The 80th percentile of the WO values is computed only on the edge pixels, making the filling threshold context-aware. This filling also helps recover water in pixels obscured by clouds or shadows in the current image. Water is added in areas that are classified as non-water (NDWI<0 here, -0.15 in raw GWW) but exceed the WO threshold.\n",
    "\n",
    "Finally, the water mask from NDWI and the filled water mask from WO are combined to produce the total water mask.\n",
    "\n",
    "*The code for the steps described above can be found in the `gww` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_mask_da, water_fill_da, total_water_da = xr.apply_ufunc(\n",
    "    gww,\n",
    "    ndwi,              # NDWI array\n",
    "    ds_wo_matched,     # Water occurrence array\n",
    "    input_core_dims=[[\"y\", \"x\"], [\"y\", \"x\"]],\n",
    "    output_core_dims=[[\"y\", \"x\"], [\"y\", \"x\"], [\"y\", \"x\"]],\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[bool, bool, bool],\n",
    "    dask_gufunc_kwargs={\"allow_rechunk\": True},\n",
    "    kwargs={\n",
    "        \"canny_sigma\": 0.7,        # Sigma for Canny edge detector\n",
    "        \"canny_low\": 0.5,          # Canny low threshold\n",
    "        \"canny_high\": 1.0,         # Canny high threshold\n",
    "        \"nonwater_thresh\": 0       # NDWI non-water threshold, used to fill water pixels from WO\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Visualize example output from Subsection 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(water_mask_da.sel(time=\"2024-05-23\").compute(), cmap='Blues')\n",
    "plt.title('Water Mask')\n",
    "# plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(water_fill_da.sel(time=\"2024-05-23\").compute(), cmap='Blues')\n",
    "plt.title('Water Fill')\n",
    "# plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(total_water_da.sel(time=\"2024-05-23\").compute(), cmap='Blues')\n",
    "plt.title('Total Water')\n",
    "# plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "In the example scene from 2024-05-23 shown in the figure above, the left panel displays the initial water detected using Otsu, the middle panel shows the water filling based on the WO dataset, and the right panel presents the total detected water produced by our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### 2.4 - Extract Largest Connected Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Even after thresholding, small water patches or noise may appear in the mask. Since we are interested in the main reservoir, we extract the **largest connected component** from the binary water mask. This isolates the primary water body while ignoring smaller, isolated water regions, giving a clean representation of the reservoir. For this we are using the `largest_connected_component` function. (Note that the largest connected component is not extracted in the original GWW algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largest Connected Component (LCC) filtering on Total Water\n",
    "lcc_da = xr.apply_ufunc(\n",
    "    largest_connected_component,\n",
    "    total_water_da,\n",
    "    input_core_dims=[[\"y\", \"x\"]],\n",
    "    output_core_dims=[[\"y\", \"x\"]],\n",
    "    vectorize=True,  # applies function to each 2D slice\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[bool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### 2.5 - Compute reservoir area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "As a last step, we can calculate the reservoir area by counting the number of pixels in the largest water body and converting this count to square kilometers using the pixel size (10 m for our case; see the `compute_area_km2` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute area in kmÂ² of LCC water bodies\n",
    "area_da = xr.apply_ufunc(\n",
    "    compute_area_km2,\n",
    "    lcc_da,\n",
    "    input_core_dims=[[\"y\", \"x\"]],\n",
    "    output_core_dims=[[]],  # scalar per time\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    kwargs={\"pixel_size\": 10.0},\n",
    "    output_dtypes=[float],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Visualize example output from Subsections 2.4 and 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(lcc_da.sel(time=\"2024-05-23\").compute(), cmap='Blues')\n",
    "plt.title(f'Largest Connected Component on 2024-05-23 \\n Calculated Area: {area_da.sel(time=\"2024-05-23\").compute():.2f} kmÂ²');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "Compute and visualize the estimated areas over time. (Note that the final surface water area time series in the raw GWW algorithm are post-processed with a temporal quantile-based filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute area time series\n",
    "area_da = area_da.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "Plotting the entire time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "area_da.to_pandas().plot(\n",
    "    marker='x', \n",
    "    ms=4,                \n",
    "    markeredgewidth=3,     \n",
    "    linestyle=' ', \n",
    "    color='darkorange', \n",
    "    label=\"Computed\"\n",
    ")\n",
    "\n",
    "plt.ylabel('Surface Area (kmÂ²)')\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrated how Sentinel-2 L1C data can be accessed and processed to monitor reservoir water surface area over a historical time series using Earth Observation techniques. To achieve this, we applied parts of the [Global Water Watch (GWW)](https://www.globalwaterwatch.earth/) algorithms (detailed description [here](https://www.nature.com/articles/s41598-022-17074-6)) to filter images, derive water masks, fill gaps and estimate reservoir surface.\n",
    "\n",
    "The workflow relied on the `.zarr` data format from the [EOPF STAC Catalog](https://stac.browser.user.eopf.eodc.eu/?.language=en), which enables fast, efficient, and scalable data access directly from the cloud, using standard Python tools. Compared to the traditional `.safe` files/workflows, this approach reduces overhead, allowing us to produce reservoir time series in just a few minutes.\n",
    "\n",
    "Overall, this demonstrates the strong potential of this catalog not only for retrospective reservoir monitoring but also for near-real-time operational applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## ðŸ’ª Now it is your turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "Now that youâ€™re familiar our reservoir detection workflow, based on the zarr format provided by the EOPF Sentinel STAC Catalog, itâ€™s time to put your skills into practice! Below are suggested exercises that scale from replication to comparison and finally to an advanced extension using a different sensor.\n",
    "\n",
    "**Overview of tasks:**\n",
    "1. **Try another reservoir** â€“ replicate the workflow for a different reservoir.  \n",
    "2. **Compare with the official GWW algorithm** â€“ analyze differences between our implementation and the GWW API results.  \n",
    "3. **Explore SAR-based water extent estimation** â€“ extend the workflow to Sentinel-1 SAR data and compare with optical results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "### Task 1: Try Another Reservoir\n",
    "\n",
    "**Goal:** test how well the workflow generalizes to a different reservoir.\n",
    "\n",
    "**Steps:**\n",
    "1. Visit the [Global Water Watch (GWW) interactive viewer](https://www.globalwaterwatch.earth/map) and explore reservoirs.  \n",
    "2. Pick a reservoir you find interesting and click **`DOWNLOAD.GEOJSON`** (bottom of the panel) to download its boundary polygon.  \n",
    "3. Use **GeoPandas** to load the GeoJSON and compute the bounding box (`bbox`) for your new study area.  \n",
    "4. Re-run this notebookâ€™s processing pipeline using that bounding box.  \n",
    "\n",
    "**Tips & ideas:**\n",
    "- Choose reservoirs in different climates (e.g., semi-arid vs. humid tropical) to observe how clouds, vegetation and seasonal changes affect detection.\n",
    "- Try several sizes: small vs. large reservoirs to see how morphological operations and the largest connected component step behave. Note that usually very large reservoirs are well measured using in-situ measurements and partial observations of surface area are less indicative of the state of the full reservoir.\n",
    " \n",
    "You can use the following code snippet to get the WO dataset directly into a data array for your AOI \n",
    "```python\n",
    "import ee\n",
    "import geemap\n",
    "\n",
    "ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "aoi = ee.Geometry.Rectangle([minx, miny, maxx, maxy])\n",
    "\n",
    "# Load and process image\n",
    "water_occurrence = (\n",
    "    ee.Image(\"JRC/GSW1_4/GlobalSurfaceWater\")\n",
    "    .select(\"occurrence\")\n",
    "    .unmask(0)\n",
    "    .divide(100)\n",
    ")\n",
    "\n",
    "ds = geemap.ee_to_xarray(\n",
    "    water_occurrence,\n",
    "    geometry=aoi,\n",
    "    scale=30,\n",
    "    crs=crs_code,)\n",
    "\n",
    "da = ds[\"occurrence\"].isel(time=0).transpose(\"Y\", \"X\").sortby(\"Y\", ascending=False).drop_vars(\"time\")\n",
    "```\n",
    "\n",
    "### Task 2: Compare with the official GWW algorithm  \n",
    "\n",
    "**Goal:** explore differences between the current implementation and the official GWW outputs.\n",
    "\n",
    "As described earlier in this notebook the GWW algorithm is more complex combining many satellite sources and parts of the classification/filtering techniques/settings are different, for example the largest connected components is not extracted. In addition the resulting surface water area time series in GWW are post-processed with\n",
    "a temporal outlier filtering (quantile-based) to remove the remaining errors. Also one major difference is that in the raw algorithm a long archive is exploited.\n",
    "\n",
    "**Steps:**\n",
    "1. Read the GWW API docs: `https://api.globalwaterwatch.earth/docs`.  \n",
    "2. Install the API:  \n",
    "   ```bash\n",
    "   pip install gwwapi\n",
    "   ```\n",
    "3. Download timeseries for Mita Hills (reservoir id 88643) using the API (or the website)\n",
    "4. Compare GWW's reported water area time series with your own estimates extracted from Sentinel-2 in this notebook.\n",
    "\n",
    "You can use the following helper function to query the GWW API:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "def get_reservoir_ts_gww(reservoir_id, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Retrieve time series data for a given reservoir ID from the GWW API.\n",
    "    start_date and end_date are strings in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.globalwaterwatch.earth/v1\"\n",
    "    url = f\"{base_url}/reservoir/{reservoir_id}/ts\"\n",
    "\n",
    "    params = {\n",
    "        \"start\": f\"{start_date}T00:00:00\",\n",
    "        \"stop\": f\"{end_date}T23:59:59\"\n",
    "    }\n",
    "\n",
    "    resp = requests.get(url, params=params)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "```\n",
    "\n",
    "### Task 3 (advanced): Explore water extent estimation with SAR Sentinel-1 Level-1 GRD\n",
    "\n",
    "**Goal:**  \n",
    "Adapt the detection workflow you developed for Sentinel-2 optical data to **Sentinel-1 SAR** data, and compare how it performs for the same reservoir.\n",
    "\n",
    "**Reasoning:**\n",
    "\n",
    "In this notebook, we used **Sentinel-2 L1C optical data** to estimate reservoir surface area. The same logic - *Otsu thresholding*, *edge detection*, *morphological filtering*, and *largest connected component extraction* - can also be applied to **Sentinel-1 SAR** data. Since SAR measures **backscatter** instead of reflectance, we canâ€™t compute spectral indices like NDWI. Instead, we use the backscatter values directly to separate wet and dry pixels: water usually appears **dark (e.g. < âˆ’16 dB)**. SAR has the advantage of being **cloud-independent**, but can sometimes overestimate water, especially when **dry soil** also produces low backscatter. Also note that SAR requires preprocessing before using it like orbit correction, thermal noise removal, calibration, speckle filtering, terrain correction, conversion to dB etc.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Download and preprocess Sentinel-1 GRD data (see the Flood Mapping - Time Series Analysis in Valencia example).  \n",
    "2. Adjust the gww function to work with SAR, e.g. apply a simple mask (e.g. `backscatter < -16`), refine with **Otsu thresholding**, use existing morphological steps to clean the result and extract the reservoir (you can skip the water occurrence filling).\n",
    "3. Compare the SAR-based water area with your Sentinel-2 estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "In the following [notebook](./68_vegetation_anomalies.ipynb), we will explore the application of Sentinel 2 L2A data for analysing vegetation anomalies in German forests.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eopf-101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
