{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Create and Visualise Multiscale Pyramids - Part 2\"\n",
    "execute:\n",
    "  enabled: true\n",
    "  keep-ipynb: false\n",
    "  freeze: auto\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "<a href= 'https://jupyterhub.user.eopf.eodc.eu/hub/login?next=/hub/spawn?next=/hub/user-redirect/git-pull?repo=https://github.com/eopf-toolkit/eopf-101&branch=main&urlpath=lab/tree/eopf-101/02_about_eopf_zarr/66_use_overviews.ipynb#fancy-forms-config={\"profile\":\"choose-your-environment\",\"image\":\"unlisted_choice\",\"image:unlisted_choice\":\"4zm3809f.c1.de1.container-registry.ovh.net/eopf-toolkit-python/eopf-toolkit-python:latest\",\"autoStart\":\"true\"}' target=\"_blank\">\n",
    "  <button style=\"background-color:#0072ce; color:white; padding:0.6em 1.2em; font-size:1rem; border:none; border-radius:6px; margin-top:1em;\">\n",
    "    üöÄ Launch this notebook in JupyterLab\n",
    "  </button>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**By:** *[@christophenoel](https://github.com/christophenoel)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Overviews enable efficient visualisation by providing progressively coarser representations of the data, allowing us to quickly navigate and explore large satellite images without loading the full-resolution dataset every time.\n",
    "\n",
    "Our approach consists of two notebooks:\n",
    "\n",
    "* [Part 1: Creating Zarr Overviews](./65_create_overviews.ipynb)\n",
    "* **Part 2: Visualising Multiscale Pyramids**\n",
    "\n",
    "In this notebook, we will explore and visualise the **multiscale overview pyramid** created in the [previous tutorial](./65_create_overviews.ipynb). Building upon what we learned about creating overviews, we will now focus on how to use them effectively for interactive visualisation and exploration of large Earth Observation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### What we will learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "* üìä How to parse and inspect the multiscales metadata structure?\n",
    "* üîç How to load different overview levels dynamically from the Zarr hierarchy?\n",
    "* üé® How to create RGB composites at different resolutions for visualisation?\n",
    "* üñ±Ô∏è How to build interactive widgets for exploring zoom levels?\n",
    "* üó∫Ô∏è How to create an interactive web map with automatic level selection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "To successfully run this notebook, make sure to have:\n",
    "\n",
    "* Created the overview hierarchy [here](./65_create_overviews.ipynb)\n",
    "* Successfully generated multiscales metadata for Sentinel-2 L2A reflectance data\n",
    "* A Zarr dataset with overview levels stored in the `measurements/reflectance/r10m/overviews/` subfolder on S3\n",
    "\n",
    "Required packages:\n",
    "\n",
    "* **Core**: `xarray`, `zarr`, `numpy`, `pandas`, `s3fs`\n",
    "* **Visualisation**: `matplotlib`, `ipywidgets`\n",
    "* **Web mapping**: `ipyleaflet`, `pyproj`, `pillow`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Import libraries\n",
    "\n",
    "Note that the below environment variables are necessary to retrieve the product with generated overviews from our private S3 bucket storage. You may edit the values to test the function with your own bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr, os, s3fs, json, pandas as pd, warnings, numpy as np, matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We prepare our credentials for S3 access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "bucket = os.environ[\"BUCKET_NAME\"]\n",
    "access = os.environ[\"ACCESS_KEY\"]\n",
    "secret = os.environ[\"SECRET_KEY\"]\n",
    "bucket_endpoint = os.environ[\"BUCKET_ENDPOINT\"]\n",
    "# S3 filesystem\n",
    "fs = s3fs.S3FileSystem(\n",
    "    key=access,\n",
    "    secret=secret,\n",
    "    client_kwargs={\"endpoint_url\": bucket_endpoint},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Understanding the Overview Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The first step in working with overviews is to understand their structure. In the previous tutorial, we created a multiscale pyramid with several levels (L0 through L7), where each level represents the data at a different resolution.\n",
    "\n",
    "Let's load the dataset and examine the **multiscales metadata** that describes this hierarchy. This metadata follows the GeoZarr Overviews specification and contains information about:\n",
    "- Which levels exist and where they are stored in the Zarr hierarchy\n",
    "- How each level relates to others (parent-child relationships through `derived_from`)\n",
    "- The spatial resolution (`cell_size`) of each level in meters\n",
    "- What resampling method was used to create each level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Opening the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We start by opening the dataset at the group level where our overviews are stored on S3. This is the same path we used in the previous tutorial when creating the overviews (`measurements/reflectance/r10m`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 path configuration\n",
    "s3_zarr_name = \"S2A_MSIL2A_20250831T135741_N0511_R010_T26WPD_20250831T185012.zarr\"\n",
    "variable_group_path = \"measurements/reflectance/r10m\"\n",
    "# Open dataset from S3 using fs.get_mapper()\n",
    "r10m_store = fs.get_mapper(f\"{bucket}/{s3_zarr_name}/{variable_group_path}\")\n",
    "dataset = xr.open_dataset(r10m_store, engine=\"zarr\")  # Load dataset\n",
    "print(f\"Dataset: {dict(dataset.dims)} | Variables: {list(dataset.data_vars)} | Multiscales: {'multiscales' in dataset.attrs}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The output confirms that our dataset has dimensions of 10,980 √ó 10,980 pixels and contains four spectral bands (b02, b03, b04, b08), along with the multiscales metadata we created in the previous tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Parsing the multiscales metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "The `multiscales` attribute contains all the information about our overview pyramid. Let's parse it into a table format using pandas to make it easier to understand.\n",
    "\n",
    "Each row in the table represents one resolution level:\n",
    "\n",
    "* **id**: Level identifier (L0, L1, L2, etc.)\n",
    "* **path**: Where this level is stored in the Zarr hierarchy (`.` means current group, `overviews/L*` means subfolder)\n",
    "* **cell_size**: Spatial resolution in metres as `[x_res, y_res]`\n",
    "* **derived_from**: Which parent level this was created from\n",
    "* **factors**: Downsampling factors applied as `[y_factor, x_factor]`\n",
    "* **resampling_method**: How pixels were aggregated (average, nearest, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse multiscales metadata\n",
    "multiscales= dataset.attrs[\"multiscales\"]\n",
    "layout_df = pd.DataFrame(multiscales[\"layout\"])\n",
    "print(\"Overview Level Hierarchy:\")\n",
    "display(layout_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Notice the hierarchical structure:\n",
    "\n",
    "* **L0** has `path=\".\"`, meaning it references the native data at the current group level (no data duplication).\n",
    "* **L1-L7** have `path=\"overviews/L*\"`, stored in the `overviews/` subfolder we created.\n",
    "* Each level is progressively coarser: L1 is 2√ó downsampled, L2 is 4√ó, L3 is 8√ó, and so on.\n",
    "* The `cell_size` doubles at each level: 10m ‚Üí 20m ‚Üí 40m ‚Üí 80m ‚Üí 160m ‚Üí 320m ‚Üí 640m ‚Üí 1280m.\n",
    "* All levels use `average` resampling, which is appropriate for continuous reflectance data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Loading Overview Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Now that we understand the structure from the metadata, let's load each overview level into memory. We'll iterate through the layout array from the multiscales metadata and open each level as a separate xarray Dataset.\n",
    "\n",
    "The code below:\n",
    "\n",
    "1. Creates an empty dictionary to store all overview datasets\n",
    "2. Loops through each entry in the layout metadata\n",
    "3. Constructs the full path by joining the base path with the level's relative path\n",
    "4. Opens each level using `xr.open_dataset()` with the Zarr engine\n",
    "5. Displays the dimensions and downsampling factor for each level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_datasets = {}  # Dictionary to store all levels\n",
    "for entry in multiscales[\"layout\"]:\n",
    "    level_id, level_path = entry[\"id\"], entry[\"path\"]\n",
    "    # Construct S3 path for each level\n",
    "    if level_path == \".\":\n",
    "        # L0 references the base dataset\n",
    "        level_store = fs.get_mapper(f\"{bucket}/{s3_zarr_name}/{variable_group_path}\")\n",
    "    else:\n",
    "        # L1-L7 are in the overviews subfolder\n",
    "        level_store = fs.get_mapper(f\"{bucket}/{s3_zarr_name}/{variable_group_path}/{level_path}\")\n",
    "    \n",
    "    overview_datasets[level_id] = xr.open_dataset(level_store, engine=\"zarr\")\n",
    "    print(f\"Loaded  {level_id}: {overview_datasets[level_id]['b02'].shape[0]:5d} √ó {overview_datasets[level_id]['b02'].shape[1]:5d} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Observe how the dimensions decrease at each level:\n",
    "\n",
    "* **L0**: 10,980 √ó 10,980 pixels (full resolution, ~121 megapixels)\n",
    "* **L1**: 5,490 √ó 5,490 pixels (half resolution, ~30 megapixels)\n",
    "* **L2**: 2,745 √ó 2,745 pixels (quarter resolution, ~7.5 megapixels)\n",
    "* **L7**: 85 √ó 85 pixels (128√ó downsampled, ~7 kilopixels)\n",
    "\n",
    "Lower resolution levels are much faster to load and visualise, making them ideal for quick previews, thumbnails, or zoomed-out views. This is the key benefit of the overview pyramid structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Visualization of Overviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Creating RGB Composites\n",
    "\n",
    "To visualise our Sentinel-2 data, we'll create **RGB true-colour composites** by combining three spectral bands:\n",
    "\n",
    "* **Red channel**: Band 4 (b04) at 664 nm - sensitive to red wavelengths\n",
    "* **Green channel**: Band 3 (b03) at 560 nm - sensitive to green wavelengths\n",
    "* **Blue channel**: Band 2 (b02) at 490 nm - sensitive to blue wavelengths\n",
    "\n",
    "However, raw reflectance values (typically ranging from 0 to ~0.4) don't display well directly. We need to apply **contrast enhancement** through percentile stretching to make features visible and create visually appealing images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Percentile stretching for contrast enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "The `normalise()` helper function below performs **percentile-based contrast stretching**, a standard technique in remote sensing visualisation. It works by:\n",
    "\n",
    "1. Filtering out NaN (fill) values, which are common in Earth Observation data due to clouds, partial coverage, or sensor issues\n",
    "2. Computing the 2nd and 98th percentile values of the valid data\n",
    "3. Linearly mapping these percentile values to 0 and 1 respectively\n",
    "4. Clipping any values outside this range\n",
    "\n",
    "This approach is robust to outliers (very bright or dark pixels) and produces visually balanced images. **Critical note**: Proper NaN handling is essential - if NaN values aren't filtered out, `np.percentile()` returns NaN, causing the entire image to display as white or blank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Normalize each channel using percentile stretch (2% - 98%)\n",
    "def normalize(band):\n",
    "    valid_pixels = band[~np.isnan(band)]  # ‚Üê ADD THIS LINE\n",
    "    if len(valid_pixels) == 0:\n",
    "        return np.zeros_like(band)\n",
    "    p2, p98 = np.percentile(valid_pixels, [2, 98])  # ‚Üê USE valid_pixels\n",
    "    if p98 == p2:\n",
    "        return np.zeros_like(band)\n",
    "    normalized = (band - p2) / (p98 - p2)\n",
    "    return np.clip(normalized, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Interactive visualization with zoom levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Now let's create an **interactive visualisation** that demonstrates the power of overviews. The widget below allows you to:\n",
    "\n",
    "* **Move the slider** to switch between zoom levels (0 = highest resolution L0, 7 = lowest resolution L7)\n",
    "* **Observe the performance difference**: Lower resolution levels (L5-L7) display almost instantly, while higher resolution levels take longer\n",
    "* **Notice the quality tradeoff**: Higher levels show more detail but require more time to load and render\n",
    "\n",
    "The function crops each image to the top-left quarter to make the display more responsive. Even with this cropping, L0 at full resolution would be 2,745 √ó 2,745 pixels, which is still quite large. This demonstrates why overviews are so valuable for interactive applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_level(i):\n",
    "    \"\"\"Show RGB composite for overview level (cropped top-left).\"\"\"\n",
    "    ds = overview_datasets[f\"L{i}\"]                         # Select overview level\n",
    "    r, g, b = [ds[b].values for b in ('b04', 'b03', 'b02')] # Extract RGB bands\n",
    "    h, w = r.shape\n",
    "    rgb = np.dstack([r, g, b])[:h//4, :w//4]                # Crop top-left quarter\n",
    "    img = np.dstack([normalize(rgb[..., j]) for j in range(3)]) # Normalise bands\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4.5))               # Set figure size\n",
    "    ax.imshow(img)                                           # Display RGB image\n",
    "    plt.show()\n",
    "interact(visualize_level, i=IntSlider(min=0, max=7, value=4, description='Zoom Level')); # Interactive slider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Comparing Multiple Resolution Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "To better understand the effect of downsampling, let's compare **multiple overview levels side by side**. This visual comparison helps us see:\n",
    "\n",
    "* **How much spatial detail is lost** at each downsampling level\n",
    "* **The tradeoff between image quality and file size/performance** - L2 is 16√ó smaller than L0 but still retains good detail\n",
    "* **Which level is appropriate for different use cases**: L2 for regional analysis, L5 for quick previews, L7 for thumbnails\n",
    "\n",
    "The function below displays three different levels (L2, L5, and L7) to show this progression from medium to very coarse resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_levels(levels=(2, 5, 7)):\n",
    "    \"\"\"Display RGB composites for selected overview levels.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(levels), figsize=(12, 4))\n",
    "    for ax, i in zip(axes, levels):\n",
    "        ds = overview_datasets[f\"L{i}\"]                              # Select overview level\n",
    "        rgb = np.dstack([ds[b].values for b in ('b04', 'b03', 'b02')]) # Stack RGB bands\n",
    "        img = np.dstack([normalize(rgb[..., j]) for j in range(3)])     # Normalise\n",
    "        ax.imshow(img); ax.axis(\"off\")\n",
    "        ax.set_title(f\"L{i} | {rgb.shape[0]}√ó{rgb.shape[1]} | √ó{2**i}\", fontsize=10)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "show_levels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Notice the progressive loss of detail:\n",
    "\n",
    "* **L2 (4√ó downsampled, 2745√ó2745 px)**: Retains good spatial detail, suitable for regional-scale visualisation and analysis\n",
    "* **L5 (32√ó downsampled, 343√ó343 px)**: Shows general patterns and major features but loses fine details like small fields or roads\n",
    "* **L7 (128√ó downsampled, 85√ó85 px)**: Provides only a coarse overview showing large-scale patterns, useful for thumbnails or global context\n",
    "\n",
    "This demonstrates why overviews are essential for **progressive rendering** in web mapping applications: the application can display L7 instantly for context, then progressively load L5, L3, L1 as the user waits, and finally load L0 when zoomed in for detailed analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Interactive Web Map with Automatic Level Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Now let's put everything together by creating a **professional-like interactive web map** using ipyleaflet. This demonstrates a real-world application of overviews for geospatial visualisation.\n",
    "\n",
    "**Key features:**\n",
    "\n",
    "* **Automatic level selection**: As you zoom in or out, the map automatically switches to the most appropriate overview level based on the current zoom level and ground resolution\n",
    "* **Manual override**: Use the slider to manually select a specific level if you want to compare quality at different resolutions\n",
    "* **Standard map controls**: Pan by clicking and dragging, zoom with mouse wheel or +/- buttons, switch basemap layers\n",
    "* **Real-time information**: The label shows which level is displayed, its dimensions, cell size, and current ground resolution\n",
    "\n",
    "The automatic selection algorithm works by:\n",
    "\n",
    "1. Calculating the ground resolution (metres per pixel) at the current Web Mercator zoom level\n",
    "2. Comparing it to the `cell_size` metadata of each overview level\n",
    "3. Selecting the level whose `cell_size` is closest to the ground resolution\n",
    "\n",
    "This ensures you always see the optimal balance between image quality and loading performance for your current view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Interactive Map with Smart Overview Level Selection\n",
    "from ipyleaflet_multiscales import create_interactive_map\n",
    "\n",
    "# Get metadata from the base level (L0)\n",
    "metadata = overview_datasets[\"L0\"].b02.attrs\n",
    "\n",
    "# Define RGB band names (Sentinel-2: R=b04, G=b03, B=b02)\n",
    "band_names = {\"r\": \"b04\", \"g\": \"b03\", \"b\": \"b02\"}\n",
    "\n",
    "# Create the interactive map\n",
    "# - initial_level=4: Start with L4 (good balance between quality and performance)\n",
    "# - initial_zoom=10: Start zoomed to show the full area\n",
    "# - band_names: Specify which bands to use for RGB composite\n",
    "map_widget = create_interactive_map(\n",
    "    overview_datasets=overview_datasets,\n",
    "    multiscales=multiscales,\n",
    "    metadata=metadata,\n",
    "    initial_level=5,\n",
    "    initial_zoom=7,\n",
    "    band_names=band_names\n",
    ")\n",
    "\n",
    "# Display the map\n",
    "# The label now shows: Level | Pixel dimensions | Cell size | Zoom | Ground resolution\n",
    "display(map_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "**How to use the interactive map:**\n",
    "\n",
    "1. **Zoom in/out** using the +/- buttons, mouse wheel, or double-click - watch as the overview level automatically adjusts to match your zoom level\n",
    "2. **Pan the map** by clicking and dragging to explore different areas\n",
    "3. **Use the slider** to manually select a specific overview level if you want to compare quality\n",
    "4. **Monitor the label** which shows:\n",
    "\n",
    "   * **Level ID and dimensions**: e.g. \"L5 (343√ó343 px)\"\n",
    "   * **Downsampling factor**: e.g. \"32√ó downsampled\"\n",
    "   * **Cell size**: The spatial resolution in metres (e.g. \"320.0m\" means each pixel represents 320m on the ground)\n",
    "   * **Current zoom**: The Web Mercator zoom level (typically 1-18)\n",
    "   * **Ground resolution**: The actual pixel size at the current zoom level (e.g. \"84.5m/px\")\n",
    "\n",
    "Try zooming in and out to see how the system automatically switches between levels to maintain both visual quality and responsiveness!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## üí™ Now it is your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Now that you've learned how to visualise and interact with multiscale overviews, try these exercises to deepen your understanding:\n",
    "\n",
    "### Task 1: Experiment with different band combinations\n",
    "\n",
    "Instead of true colour RGB (b04, b03, b02), try creating **false colour composites** that highlight different features:\n",
    "\n",
    "* **Colour infrared (CIR)**: `{\"r\": \"b08\", \"g\": \"b04\", \"b\": \"b03\"}` - highlights vegetation in red tones\n",
    "* **Agriculture composite**: `{\"r\": \"b08\", \"g\": \"b03\", \"b\": \"b02\"}` - emphasises crop health and vigour\n",
    "* **Urban analysis**: `{\"r\": \"b08\", \"g\": \"b04\", \"b\": \"b02\"}` - distinguishes urban areas from vegetation\n",
    "\n",
    "Modify the `band_names` dictionary in the map creation code above and re-run the cell to see how different band combinations reveal different information about the landscape.\n",
    "\n",
    "### Task 2: Apply this workflow to a different area\n",
    "\n",
    "Go back to the previous tutorial ([65_create_overviews.ipynb](65_create_overviews.ipynb)) and create overviews for a different Sentinel-2 scene from your region of interest. Then return to this notebook and update the `s3_zarr_name` variable to visualise your new dataset. Compare how the overview structure works for different landscapes (urban vs rural, mountainous vs flat, etc.).\n",
    "\n",
    "### Task 3: Analyse the performance tradeoff\n",
    "\n",
    "Measure how long it takes to load and display different overview levels. Add timing code like:\n",
    "\n",
    "```python\n",
    "import time\n",
    "start = time.time()\n",
    "visualize_level(0)  # L0 - full resolution\n",
    "print(f\"L0 took {time.time()-start:.2f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "visualize_level(5)  # L5 - much coarser\n",
    "print(f\"L5 took {time.time()-start:.2f} seconds\")\n",
    "```\n",
    "\n",
    "How much faster is L5 compared to L0? What about L7? At what level do you think the quality becomes too low for useful analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Now that you've mastered creating and visualising GeoZarr overviews through tutorials 65 and 66, you can:\n",
    "\n",
    "* **Apply these techniques to your own datasets**: Use the complete workflow on your Earth Observation data from any source (Sentinel-2, Landsat, commercial satellites, etc.)\n",
    "* **Build custom web applications**: The `ipyleaflet_multiscales` module provides a foundation for developing interactive mapping tools tailored to specific needs\n",
    "* **Optimise for your use case**: Test different chunk sizes, compression algorithms, and scale factors to find the best balance between file size and access performance\n",
    "* **Scale to cloud platforms**: The workflow demonstrated here already uses S3 cloud object storage, which enables web-scale access and can be adapted to other cloud providers (Azure Blob, Google Cloud Storage)\n",
    "\n",
    "**Further resources:**\n",
    "\n",
    "* [GeoZarr Specification](https://github.com/zarr-developers/geozarr-spec) - Full technical specification for GeoZarr extensions\n",
    "* [xarray documentation](https://docs.xarray.dev/) - Comprehensive guide to working with labelled multi-dimensional arrays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "In this tutorial, we've learned how to **visualise and interact with multiscale overview pyramids** for GeoZarr datasets. We covered:\n",
    "\n",
    "1. **Understanding the hierarchy**: How to parse and inspect multiscales metadata following the GeoZarr specification\n",
    "2. **Loading levels dynamically**: How to iterate through the layout metadata and load different resolution levels\n",
    "3. **Creating RGB composites**: How to combine spectral bands with proper contrast enhancement and NaN handling\n",
    "4. **Interactive exploration**: How to build responsive widgets for exploring different zoom levels\n",
    "5. **Professional web mapping**: How to create maps with automatic level selection based on zoom and ground resolution\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "* Overviews enable **efficient multi-scale visualisation** by providing progressively coarser representations of large datasets\n",
    "* **Automatic level selection** ensures optimal performance while maintaining visual quality appropriate for the current view\n",
    "* **Proper NaN handling** is critical - failing to filter NaN values before percentile calculations causes white or blank displays\n",
    "* The `cell_size` metadata enables **intelligent zoom-aware rendering** by matching overview resolution to ground resolution\n",
    "\n",
    "These techniques are essential for building **interactive Earth Observation applications** that remain responsive even when working with very large satellite imagery datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "In the following [notebook](./67_reservoir_surface_monitoring.ipynb), we will go South and showcase the application of Sentinel 2 L1C data, focusing on Water Reservoirs.<br>\n",
    "We will **implement parts of the GWW algorithms** to estimate water surface area for a single reservoir: **Mita Hills in Zambia**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eopf-101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
