{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Fire in Sardinia 2025 - Part 1\"\n",
    "execute:\n",
    "  enabled: true\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "<a href=\"https://jupyterhub.user.eopf.eodc.eu/hub/login?next=%2Fhub%2Fspawn%3Fnext%3D%252Fhub%252Fuser-redirect%252Fgit-pull%253Frepo%253Dhttps%253A%252F%252Fgithub.com%252Feopf-toolkit%252Feopf-101%2526branch%253Dmain%2526urlpath%253Dlab%252Ftree%252Feopf-101%252F06_eopf_zarr_in_action%252F61_sardinia_s2_tfci.ipynb%23fancy-forms-config=%7B%22profile%22%3A%22choose-your-environment%22%2C%22image%22%3A%22unlisted_choice%22%2C%22image%3Aunlisted_choice%22%3A%224zm3809f.c1.de1.container-registry.ovh.net%2Feopf-toolkit-python%2Feopf-toolkit-python%3Alatest%22%2C%22autoStart%22%3A%22true%22%7D\" target=\"_blank\">\n",
    "  <button style=\"background-color:#0072ce; color:white; padding:0.6em 1.2em; font-size:1rem; border:none; border-radius:6px; margin-top:1em;\">\n",
    "    üöÄ Launch this notebook in JupyterLab\n",
    "  </button>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "**By:** *[@gisromerocandanedo](https://github.com/gisromerocandanedo)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Communities and ecosystems worldwide are under **increasing threat** from **wildfires**, a problem that is being made worse by **climate change**.<br>\n",
    "\n",
    "Monitoring these events is crucial, and **satellite imagery** is an invaluable tool in this effort.\n",
    "The **Sentinel satellite missions** offer valuable insights into the different stages of a fire. A **True Colour Composite** of Sentinel-2 data can be used to track smoke, **False Colour Composites** can clearly distinguish healthy vegetation from damaged or burnt areas. Furthermore, True- or False color composites can be combined with other data sources, such as Land Surface Temperature (LST) from **Sentinel-3**. Temperature anomalies retrieved from Sentinel-3 data are useful information to get a more comprehensive picture of a fire event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### The Fire Event\n",
    "\n",
    "According to the [Italian National Institute of Health (ISS)](https://alleanzacontroilcancro.it/en/istituto/istituto-istituto-superiore-sanita/), between January and July 2025, Italy experienced a significant number of wildfires, with approximately 700 recorded incidents burning over 320 square kilometres.<br>\n",
    "\n",
    "We will focus on a specific fire that occurred in the **Province of Nuoro** in the Italian region of **Sardinia**, on **June 10th, 2025**. The [European Forest Fire Information System (EFFIS)](https://forest-fire.emergency.copernicus.eu/?pk_vid=1c5add39a04f87ba17556907226604b6) estimated that it consumed an estimated 1000 hectares. Based on this area, we can expect it to be clearly visible in Sentinel-2 imagery with a 20-metre resolution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### The case study\n",
    "\n",
    "This case study consists of a **series** of **three notebooks**:\n",
    "\n",
    "* **Part 1 - Compare Sentinel-2 True- and False-Color composites before and after a fire event**\n",
    "* [Part 2 - Analyse fire intensity with Sentinel-2 and -3 data](./62_sardinia_s3_lst.ipynb)\n",
    "* [Part 3 - Assess burn severity with the normalised burn ratio (dNBR)](./63_sardinia_dNBR.ipynb)\n",
    "\n",
    "The three notebooks will guide you through a comprehensive **remote sensing workflow** for **monitoring** **wildfires** covering all stages of a **fire life cycle**: **before**, **during**, and **after** an event. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### About this notebook\n",
    "\n",
    "This notebook will take you through a practical example of how you can use Sentinel-2 data to visually compare an area before and after a wildfire event.<br>\n",
    "\n",
    "We will access Sentinel-2 data available at the [EOPF STAC Catalog](https://stac.browser.user.eopf.eodc.eu/?.language=en) to compare the landscape before and after wildfire event. Such a comparison allows us to determine the extent of the damage caused by the wildfire. This comparison can be done by using different combinations of spectral bands to identify **healthy** vs. **damaged** vegetation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### What we will learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "- üí® Create cloud-free layers from Sentinel-2 L2A data for a clear view of the surface at a fire location.\n",
    "- ‚úÇÔ∏è Extract and clip specific areas of interest from satellite imagery using a STAC catalog and reprojected bounding boxes.\n",
    "- üõ∞Ô∏è Set a starting point for wildfire events analysis by visualising vegetation and land conditions using Sentinel-2 L2A imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import LocalCluster\n",
    "from pystac_client import Client\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Transformer\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from skimage import exposure\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "from shapely.geometry import box "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "This notebook makes use of a set of functions that are all listed inside the [zarr_wf_utils.py](./zarr_wf_utils.py) script. Inside the script, we will find costumised functions that allow us to mask, normalise and extract specific areas of our items of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our utility functions\n",
    "\n",
    "from zarr_wf_utils import (\n",
    "    validate_scl,\n",
    "    mask_sub_utm,\n",
    "    normalisation_str_gm,\n",
    "    lat_lon_to_utm_box,\n",
    "    zarr_mask_utm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Defining parameters for querying the EOPF STAC catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "As a first step, we need to define specific parameters for our query, including:<br>\n",
    "\n",
    "* two **key dates** for our comparison: one date for a **pre-fire** view, **3rd of June 2025** (one week before the event) and a second for the **post-fire** view on **21st June 2025** (10 days after)\n",
    "* the **data collection** we are interested in, and\n",
    "* **bounding box** information for the area of interest.\n",
    "\n",
    "In addition, we set the appropriate re-projection parameters to ensure a smooth and efficient workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The timeframe and area of interest for our filtering\n",
    "def_collection = 'sentinel-2-l2a' # collection\n",
    "# Before the fire:\n",
    "pre_f  = '2025-06-03'\n",
    "# After the fire:\n",
    "post_f = '2025-06-21'\n",
    "\n",
    "search_bbox = (8.847198,40.193395,8.938865,40.241895)\n",
    "\n",
    "# Definition of the transformer parameters from lat/lon to UTM that ensure\n",
    "# correct overlay of layers\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32632\", always_xy=True)\n",
    "\n",
    "t_utm_to_deg = Transformer.from_crs(\"EPSG:32632\",\"EPSG:4326\", always_xy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Initiate a Dask cluster\n",
    "In a next step we initiate a **virtual Dask cluster**. This cluster consists of a scheduler (the \"brain\") and several workers (the \"hands\"), which enables faster processing of large datasets by breaking down tasks and running them in parallel. <br>\n",
    "A client is then created to manage communication between the code and this cluster.\n",
    "\n",
    "For more information, feel free to visit the **dask** [documentation](https://docs.dask.org/en/stable/) and the tutorial [How to use dask](http://docs.dask.org/en/stable/#how-to-use-dask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track the the performance the code will have\n",
    "st = time.time()\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Establish a connection to the EOPF STAC Catalog\n",
    "Data is retrieved from the endpoint of the [EOPF STAC Catalog](https://stac.browser.user.eopf.eodc.eu/?.language=en). We can do this with the function `Client.open()` from the `pystac_client` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "eopf_stac_api_root_endpoint = \"https://stac.core.eopf.eodc.eu/\" #root starting point\n",
    "eopf_catalog = Client.open(url=eopf_stac_api_root_endpoint) # calls the selected url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Pre-Fire Visualisation\n",
    "\n",
    "The first step is to create a **visual representation** of our area of interest for a day before the fire occured: 3 June 2025.\n",
    "We will use the parameters we defined at the beginning to query the STAC collection and retrieve the specific **pre-fire** images needed for our analysis. \n",
    "\n",
    "With the `search()` function, you can query a STAC catalog based on given keyword arguments. As a result of the search, you see that one Item is returned from the STAC Catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interest timeframe parameters for the filtering\n",
    "date_pre = pre_f + 'T00:00:00Z/' + pre_f + 'T23:59:59.999999Z' # interest period\n",
    "s2_col = list(eopf_catalog.search(\n",
    "                bbox= search_bbox, # area\n",
    "                datetime= date_pre, #time frame\n",
    "                collections=def_collection # collection\n",
    "                ).item_collection())\n",
    "\n",
    "av_urls = [item.assets[\"product\"].href for item in s2_col]  # stores the available Items URLs\n",
    "print(\"Search Results:\")\n",
    "print('Total Items Found for Sentinel-2 L2A over Sardinia:  ',len(av_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Now, we can retrieve the item and directly and open it as a `xarray.DataTree`. In addition, key information from the item's properties is also extracted to verify key properties of the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are interested in the datasets contained in the measurements bands for True Colour and False Colour Composites.\n",
    "s2_zarr = xr.open_datatree(\n",
    "    av_urls[0], engine=\"zarr\", #we always get the earliest one (the first available item goes last)\n",
    "    chunks={},\n",
    "    decode_timedelta=False\n",
    "    )\n",
    "\n",
    "# Store interest parameters for further plotting:\n",
    "date = s2_zarr.attrs['stac_discovery']['properties']['start_datetime'][:10]\n",
    "time_zarr = s2_zarr.attrs['stac_discovery']['properties']['start_datetime'][11:19]\n",
    "# target_crs = s2_zarr.attrs[\"stac_discovery\"][\"properties\"][\"proj:epsg\"]\n",
    "\n",
    "print(f'Item for the {date} at {time_zarr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The spectral bands we will need for our analysis are found within the `/reflectance` group, specifically under the `r20m` subgroup.<br>\n",
    "\n",
    "Before directly processing our assets of interest, we will filter out invalid pixels, such as pixels containing clouds, cloud shadows, or areas with no data. We will use the **Scene Classification Layer** (SCL) available inside the `.zarr` item, which is a separate asset located under the `/classification/r20m` group.\n",
    "\n",
    "By using the pre-defined `validate_scl()` function, we can create a boolean mask that masks out invalid pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the resolution group we are interested to analyse over:\n",
    "zarr_meas = s2_zarr.measurements.reflectance.r20m\n",
    "\n",
    "# Extract the Scene Classification Layer at 20m resolution:\n",
    "l2a_class_20m = s2_zarr.conditions.mask.l2a_classification.r20m.scl\n",
    "\n",
    "# Apply the function `validatae_scl` to mask out invalid pixels\n",
    "valid_mask = validate_scl(l2a_class_20m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "The visualisation we are intending to create covers a larger extent than the specific fire area. This helps us to better understand the event‚Äôs overall spatial extent. For this, we generate a bounding box to visually pinpoint the fire‚Äôs precise location within a wider composite image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a larger bounding box for better visualisation:\n",
    "bbox_vis = (8.649555,40.073583,9.127893,40.343840)\n",
    "\n",
    "# A fixed geographic bounding box to highlight the AOI in the map format\n",
    "map_box = search_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "In next step, we need to reproject the area of interest from EPSG: **4326** to UTM coordinates.\n",
    "Once the area is defined, our loaded data (`zarr_meas`) is masked using the defined `bbox_utm`. After extracting the area, only the valid pixels are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new list with the converted UTM coordinates\n",
    "bbox_utm = lat_lon_to_utm_box((bbox_vis[0], bbox_vis[1]),(bbox_vis[2], bbox_vis[3]),transformer)\n",
    "\n",
    "# Use the box() function to create a polygon from the coordinates\n",
    "map_box = box(map_box[0],map_box[1],map_box[2],map_box[3])\n",
    "\n",
    "# Boolean mask for the 'x' dimension (longitude/easting)\n",
    "x_mask = (zarr_meas['x'] >= bbox_utm[0]) & (zarr_meas['x'] <= bbox_utm[2])\n",
    "# Boolean mask for the 'y' dimension (latitude/northing)\n",
    "y_mask = (zarr_meas['y'] >= bbox_utm[1]) & (zarr_meas['y'] <= bbox_utm[3])\n",
    "\n",
    "# Combined mask for the bounding box\n",
    "bbox_mask = x_mask & y_mask\n",
    "\n",
    "# Extract row and column indices where the mask is True\n",
    "cols,rows = np.where(bbox_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Pre-Fire True Colour Image\n",
    "\n",
    "Once we have created the necessary masks, we can proceed to create True Color Image composites. For the composite creation, `zarr_meas` contains the assets we are interested in. The TCI composite makes use of the **red** (B04), **green** (B03), and **blue** (B02) bands to create a view that looks natural to the human eye.\n",
    "\n",
    "xarray's `where()` function allows us to apply the SLC filtering we had previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True colour channels we are interested to retrieve composite:\n",
    "tc_red  = 'b04'\n",
    "tc_green= 'b03'\n",
    "tc_blue = 'b02'\n",
    "\n",
    "# The tc_red, tc_green, and tc_blue variables are inputs specifying the band names\n",
    "red = zarr_meas[tc_red].where(valid_mask)\n",
    "gre = zarr_meas[tc_green].where(valid_mask)\n",
    "blu = zarr_meas[tc_blue].where(valid_mask)\n",
    "\n",
    "# Visualising the green band:\n",
    "plt.imshow(gre)\n",
    "plt.title('Green Reflectance (b03)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "The next step is to clip the retrieved asset to our area of interest which we defined earlier with specific bounding box information.\n",
    "We can apply the `mask_sub_utm()` function, which will masks the three bands to the defined bounding box.\n",
    "\n",
    "It is important to point out that until now, we have not accessed on disk the data. Once we add the `.values` argument to the resulting _ from `mask_sub_utm()`, the data is accessed.\n",
    "\n",
    "`.zarr` contains **x** and **y** information for each of the pixels, and storing these values for a further geolocalisation of our item of interest is essential.\n",
    "\n",
    "You can visualise the **blue** (B02) band to verify the clipped area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mask_sub_utm() function takes the bands and masks them to the valid rows and columns from the bbox_mask\n",
    "red = mask_sub_utm(red,rows, cols).values\n",
    "gre = mask_sub_utm(gre,rows, cols).values\n",
    "blu = mask_sub_utm(blu,rows, cols).values\n",
    "\n",
    "# The zarr_meas group is the input dataset containing the dimensions\n",
    "# by slicing the 'y' dimension array based on the minimum and maximum row indices\n",
    "y_zarr = zarr_meas['y'].isel(y=slice(rows.min(), rows.max() + 1)).values\n",
    "# also, the same for the 'x' dimension array based on the minimum and maximum column indices\n",
    "x_zarr = zarr_meas['x'].isel(x=slice(cols.min(), cols.max() + 1)).values\n",
    "\n",
    "# We transform the original x,y bbox from our clip to decimal degree coordinates\n",
    "map_ext_deg_pre = list(t_utm_to_deg.transform(np.nanmin(x_zarr),np.nanmin(y_zarr)) + \n",
    "                   t_utm_to_deg.transform(np.nanmax(x_zarr),np.nanmax(y_zarr)))\n",
    "\n",
    "# Visualising the clipped red band:\n",
    "plt.imshow(blu)\n",
    "plt.title('Clipped Blue Reflectance (b02)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "To create the composite image, we need to **normalise** each of the input assets. Normalisation ensures that the bands have a consistent and predictable range of values. This supports optimal data processing and removes the influence of external factors (like changing light conditions) allowing for a meaningful comparison among generated composites.\n",
    "\n",
    "The `normalisation_str_gm()` function achieves this by scaling the reflectance values to a standard range (0-255) using the percentile-based method.\n",
    "\n",
    "Once the values for our three bands have been normalised, they can be stacked in an **RGB** format to generate the initial True Colour Image (TCI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: percentile range for contrast stretching\n",
    "contrast_stretch_percentile=(2, 98)\n",
    "# Input: gamma correction value\n",
    "gamma=1.8\n",
    "\n",
    "# Apply normalisation to the red, green and blue bands using the specified percentile and gamma values\n",
    "red_processed = normalisation_str_gm(red, *contrast_stretch_percentile, gamma)\n",
    "green_processed = normalisation_str_gm(gre, *contrast_stretch_percentile, gamma)\n",
    "blue_processed = normalisation_str_gm(blu, *contrast_stretch_percentile, gamma)\n",
    "\n",
    "# We stack the processed red, green, and blue arrays\n",
    "rgb_composite_sm = np.dstack((red_processed, green_processed, blue_processed)).astype(np.float32)\n",
    "\n",
    "plt.imshow(rgb_composite_sm)\n",
    "plt.title('RGB Composite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "The image is currently displayed with a neutral colour ramp and with the non-valid masked pixels. Some of the details can be enhanced based on the information the overall composite contains.\n",
    "\n",
    "For this, we can apply a **histogram equalisation**. This technique will adjust the brightness and improve the visibility of details within our image. <br>\n",
    "Through the `skimage` library, we can apply the `exposure.equalize_adapthist()` function. This method creates a more natural-looking and visually balanced composite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding equalisation from skimage:\n",
    "fire_tc = exposure.equalize_adapthist(rgb_composite_sm)\n",
    "\n",
    "plt.imshow(fire_tc)\n",
    "plt.title('Equalised Composite') # Add a title for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Pre-Fire False Colour Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Next, a **False Colour Image** (FCI) is created to provide a clearer overview of vegetation health.<br>\n",
    "\n",
    "This image uses the **Shortwave Infrared** (B12), **Near-Infrared** (B8a), and **Blue** (B02) bands. This specific combination enhances the distinction between **healthy vegetation**, which appears **green**, and **damaged** or **burnt** areas, which are shown in vivid, **contrasting colours**.\n",
    "\n",
    "This False Colour composite will help us to better highlight the full extent of the fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The false colour channels we are interested to retrieve coposite:\n",
    "fc_swir = 'b12'\n",
    "fc_nir =  'b8a'\n",
    "fc_blue = 'b02'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Following the same principle as of the creation of the True Colour composite, we can choose the relevant bands and apply the masking and clipping steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The zarr_meas object is the input dataset containing the bands, fc_red, fc_green, fc_blue specify the bands\n",
    "# The where() method is used to apply the boolean valid_mask to the bands\n",
    "swir = zarr_meas[fc_swir].where(valid_mask)\n",
    "nir =  zarr_meas[fc_nir].where(valid_mask)\n",
    "blu =  zarr_meas[fc_blue].where(valid_mask)\n",
    "\n",
    "# The mask_sub_utm() function takes the bands and masks them to the specified rows and columns\n",
    "swir = mask_sub_utm(swir,rows, cols).values\n",
    "nir = mask_sub_utm(nir,rows, cols).values\n",
    "blu = mask_sub_utm(blu,rows, cols).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Then, we can apply the normalisation function, followed by the stacking of the three bands. The False Colour composite $$$ explain the colouring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the normalisation function to each band\n",
    "swir_processed = normalisation_str_gm(swir, *contrast_stretch_percentile, gamma)\n",
    "nir_processed = normalisation_str_gm(nir, *contrast_stretch_percentile, gamma)\n",
    "blue_processed = normalisation_str_gm(blu, *contrast_stretch_percentile, gamma)\n",
    "\n",
    "# Use np.dstack to create a false-colour composite from the processed bands\n",
    "false_composite= np.dstack((swir_processed, nir_processed, blue_processed)).astype(np.float32)\n",
    "\n",
    "# Output:\n",
    "plt.imshow(false_composite)\n",
    "plt.title('False Composite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "We then continue and also apply the equalisation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply adaptive histogram equalisation to enhance contrast for fire detection\n",
    "fire_fc = exposure.equalize_adapthist(false_composite)\n",
    "\n",
    "# Create a figure to plot\n",
    "\n",
    "plt.imshow(fire_fc)\n",
    "plt.title('Equalised False Composite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## Post-Fire Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Now, we will replicate the same visualisation for a specific time after the fire: 10 June 2025.\n",
    "Considering this new date, we will retrieve images from the same collection that correspond to our new timeframe of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_post = post_f + 'T00:00:00Z/' + post_f + 'T23:59:59.999999Z' # interest period\n",
    "\n",
    "s2_post = list(eopf_catalog.search(\n",
    "                bbox= search_bbox, \n",
    "                datetime= date_post,\n",
    "                collections=def_collection).item_collection())\n",
    "\n",
    "av_urls = [item.assets[\"product\"].href for item in s2_post]\n",
    "av_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "We open again the the retrieved item from our filtered results, followed by the masking and validation steps to ensure a clean, cloud-free composite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_zarr = xr.open_datatree(\n",
    "    av_urls[0], engine=\"zarr\", #we always get the earliest one (last)\n",
    "    chunks={},\n",
    "    decode_timedelta=False\n",
    "    )\n",
    "\n",
    "zarr_meas = post_zarr.measurements.reflectance.r20m\n",
    "\n",
    "# Extractthe cloud free mask at 20m resolution:\n",
    "l2a_class_20m = post_zarr.conditions.mask.l2a_classification.r20m.scl\n",
    "\n",
    "valid_mask = validate_scl(l2a_class_20m)  # Boolean mask (10980x10980)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### Post-Fire True Colour Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Once invalid pixels are filtered out, we can generate the corresponding True-Color composite to get a view of our area of interest after the fire event. We again clip the retrieved assets to our specific bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UTM masks for rows and columns based on the bounding box.\n",
    "cols_post, rows_post = zarr_mask_utm(bbox_utm, zarr_meas)\n",
    "\n",
    "# Select the red, green, and blue bands and apply the valid mask.\n",
    "red = zarr_meas[tc_red].where(valid_mask)\n",
    "gre =  zarr_meas[tc_green].where(valid_mask)\n",
    "blu =  zarr_meas[tc_blue].where(valid_mask)\n",
    "\n",
    "# Mask and clip the selected bands using the row and column indices.\n",
    "red = mask_sub_utm(red,rows_post, cols_post).values\n",
    "gre = mask_sub_utm(gre,rows_post, cols_post).values\n",
    "blu = mask_sub_utm(blu,rows_post, cols_post).values\n",
    "\n",
    "# Slice the y-dimension values using the new row indices.\n",
    "y_zarr = zarr_meas['y'].isel(y=slice(rows_post.min(), rows_post.max() + 1)).values\n",
    "# Slice the x-dimension values using the new column indices.\n",
    "x_zarr = zarr_meas['x'].isel(x=slice(cols_post.min(), cols_post.max() + 1)).values\n",
    "\n",
    "# We transform the original x,y bbox from our clip to decimal degree coordinates\n",
    "map_ext_deg_post = list(t_utm_to_deg.transform(np.nanmin(x_zarr),np.nanmin(y_zarr)) + \n",
    "                   t_utm_to_deg.transform(np.nanmax(x_zarr),np.nanmax(y_zarr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "And, once the new area is defined, we normalise, stack and equalize the composite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalisation and gamma correction to the red, green, and blue bands.\n",
    "red_processed = normalisation_str_gm(red, *contrast_stretch_percentile, gamma)\n",
    "green_processed = normalisation_str_gm(gre, *contrast_stretch_percentile, gamma)\n",
    "blue_processed = normalisation_str_gm(blu, *contrast_stretch_percentile, gamma)\n",
    "\n",
    "# Stack the processed bands to create a single true-colour composite image.\n",
    "rgb_composite_sm = np.dstack((red_processed, green_processed, blue_processed)).astype(np.float32)\n",
    "\n",
    "# Apply adaptive histogram equalisation to enhance the composite image.\n",
    "tc_post = exposure.equalize_adapthist(rgb_composite_sm)\n",
    "\n",
    "plt.imshow(tc_post)\n",
    "plt.title('Equalised Composite') # Add a title for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### Post-Fire False Colour Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "And as the last processing step, we create the False Colour composite for the same day, to clearly visualise the extent of the burn scars and vegetation recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the red, green, and blue bands and apply the valid mask.\n",
    "\n",
    "swir = zarr_meas[fc_swir].where(valid_mask)\n",
    "nir =  zarr_meas[fc_nir].where(valid_mask)\n",
    "blu =  zarr_meas[fc_blue].where(valid_mask)\n",
    "\n",
    "# Mask and clip the selected bands using the row and column indices.\n",
    "\n",
    "swir = mask_sub_utm(swir,rows_post, cols_post).values\n",
    "nir = mask_sub_utm(nir,rows_post, cols_post).values\n",
    "blu = mask_sub_utm(blu,rows_post, cols_post).values\n",
    "\n",
    "# Apply normalisation and gamma correction to the red, green, and blue bands.\n",
    "\n",
    "swir_processed = normalisation_str_gm(swir, *contrast_stretch_percentile, gamma)\n",
    "nir_processed = normalisation_str_gm(nir, *contrast_stretch_percentile, gamma)\n",
    "blue_processed = normalisation_str_gm(blu, *contrast_stretch_percentile, gamma)\n",
    "\n",
    "\n",
    "# Stack the processed bands to create a single false-colour composite image.\n",
    "false_composite= np.dstack((swir_processed, nir_processed, blue_processed)).astype(np.float32)\n",
    "\n",
    "# Apply adaptive histogram equalisation to enhance the composite image.\n",
    "fc_post = exposure.equalize_adapthist(false_composite)\n",
    "\n",
    "plt.imshow(fc_post)\n",
    "plt.title('Equalised Composite') # Add a title for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "## Compare pre- and post-fire composites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "As a last step, we will **georeference** and **visualise** the created composites together, presenting it in a way that makes it easier to recognise and understand the monitored areas.\n",
    "\n",
    "We will use the `cartopy` library to pinpoint the location of our georeferenced datasets based on their CRS and geospatial bounding box. The visualisation will be a 2x2 matrix, offering a comprehensive, multi-channel overview.<br>\n",
    "\n",
    "Finally, the plot will be enhanced by adding key elements such as grid lines for improved geolocation, a clear title, and a bounding box to highlight the specific area of the fire. This approach allows for a direct visual comparison of the landscape's state before and after the event.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ll = ccrs.PlateCarree()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 9), subplot_kw={'projection': data_ll})\n",
    "\n",
    "# Pre fire TCI\n",
    "ax1 = axs[0,0]\n",
    "ax1.imshow(fire_tc, origin='upper',\n",
    "                extent=[map_ext_deg_pre[0],map_ext_deg_pre[2],\n",
    "                        map_ext_deg_pre[1],map_ext_deg_pre[3]],  # item\n",
    "                transform=data_ll)\n",
    "ax1.add_geometries(map_box, crs=data_ll, facecolor='none', \n",
    "                   edgecolor='yellow', linewidth=2, linestyle='-')\n",
    "ax1.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False) # gridlines and labels\n",
    "ax1.set_title(f'TCI for {pre_f}')\n",
    "\n",
    "# Pre fire FCI\n",
    "ax2 = axs[0,1]\n",
    "ax2.imshow(fire_fc, origin='upper',\n",
    "                extent=[map_ext_deg_pre[0],map_ext_deg_pre[2],\n",
    "                        map_ext_deg_pre[1],map_ext_deg_pre[3]],  # item\n",
    "                transform=data_ll)\n",
    "ax2.add_geometries(map_box, crs=data_ll, facecolor='none', \n",
    "                   edgecolor='yellow', linewidth=2, linestyle='-')\n",
    "ax2.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False) # gridlines and labels\n",
    "ax2.set_title(f'FCI for {pre_f}')\n",
    "\n",
    "#Post fire TCO\n",
    "ax3 = axs[1,0]\n",
    "ax3.imshow(tc_post, origin='upper',\n",
    "                extent=[map_ext_deg_post[0],map_ext_deg_post[2],\n",
    "                        map_ext_deg_post[1],map_ext_deg_post[3]],  # item\n",
    "                transform=data_ll)\n",
    "ax3.add_geometries(map_box, crs=data_ll, facecolor='none', \n",
    "                   edgecolor='yellow', linewidth=2, linestyle='-')\n",
    "ax3.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False) # gridlines and labels\n",
    "ax3.set_title(f'TCI for {post_f}')\n",
    "\n",
    "# Post fire FCI\n",
    "ax4 = axs[1,1]\n",
    "ax4.imshow(fc_post, origin='upper',\n",
    "                extent=[map_ext_deg_post[0],map_ext_deg_post[2],\n",
    "                        map_ext_deg_post[1],map_ext_deg_post[3]],  # item\n",
    "                transform=data_ll)\n",
    "ax4.add_geometries(map_box, crs=data_ll, facecolor='none', \n",
    "                   edgecolor='yellow', linewidth=2, linestyle='-')\n",
    "ax4.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False) # gridlines and labels\n",
    "ax4.set_title(f'FCI for {post_f}')\n",
    "\n",
    "# Adjust the layout to prevent titles from overlapping\n",
    "fig.suptitle(f'Sentinel-2 L2A TCI and FCI', fontsize=16)\n",
    "# Display the combined plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## Calculating processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "et = time.time()\n",
    "\n",
    "total_t = et - st\n",
    "\n",
    "print('Total Running Time: ', total_t,' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "As our plots show, the **True Colour Image** reveals a clear change in the state of the vegetation, with an evident burn scar visible on the ground. The **False Colour Image** also highlights a significant change in the spectral response, which precisely encloses the spot where the fire occurred.<br>\n",
    "\n",
    "It is important to note the efficiency of this monitoring workflow. The entire process from defining the area of interest to searching, accessing, processing, and visualising the data takes less than a minute, without the need to download data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "Having processed the key spectral bands, we have successfully established a visual baseline for **our monitoring workflow**.<br>\n",
    "\n",
    "The generation of these composites allows us to quickly gain an overview of the spatial changes resulting from a fire event, as observed by **Sentinel-2 L-2A**.\n",
    "\n",
    "This workflow is **highly replicable**, enabling us to replicate these defined steps at various points in time. This ability to generate consistent visualisations will allow us to understand the full dynamics of a **fire‚Äôs event lifecycle** over time, from its beginning to the subsequent recovery of the landscape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "## What‚Äôs next?\n",
    "\n",
    "In the [following notebook](./62_sardinia_s3_lst.ipynb), we will apply the workflow we have generated to create a **True Colour Image** from **Sentinel-2 L2A** data for the day of the fire.\n",
    "\n",
    "To obtain a more detailed overview of the fire's state, we will integrate a new dataset into our workflow: **Sentinel-3** data. This will enable us to analyse thermal information and pinpoint the active fire's location."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eopf_env_wf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
